{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_data = pd.read_hdf('data.h5', 'table', where=['index>2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>lepton pt</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344727</td>\n",
       "      <td>-0.876465</td>\n",
       "      <td>0.936035</td>\n",
       "      <td>1.992188</td>\n",
       "      <td>0.882324</td>\n",
       "      <td>1.786133</td>\n",
       "      <td>-1.646484</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678223</td>\n",
       "      <td>-1.360352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946777</td>\n",
       "      <td>1.028320</td>\n",
       "      <td>0.998535</td>\n",
       "      <td>0.728516</td>\n",
       "      <td>0.869141</td>\n",
       "      <td>1.026367</td>\n",
       "      <td>0.958008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105469</td>\n",
       "      <td>0.321289</td>\n",
       "      <td>1.522461</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>-1.205078</td>\n",
       "      <td>0.681641</td>\n",
       "      <td>-1.070312</td>\n",
       "      <td>-0.921875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373535</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755859</td>\n",
       "      <td>1.361328</td>\n",
       "      <td>0.986816</td>\n",
       "      <td>0.837891</td>\n",
       "      <td>1.132812</td>\n",
       "      <td>0.872070</td>\n",
       "      <td>0.808594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.595703</td>\n",
       "      <td>-0.607910</td>\n",
       "      <td>0.007076</td>\n",
       "      <td>1.818359</td>\n",
       "      <td>-0.111877</td>\n",
       "      <td>0.847656</td>\n",
       "      <td>-0.566406</td>\n",
       "      <td>1.581055</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654297</td>\n",
       "      <td>-1.274414</td>\n",
       "      <td>3.101562</td>\n",
       "      <td>0.823730</td>\n",
       "      <td>0.937988</td>\n",
       "      <td>0.971680</td>\n",
       "      <td>0.789062</td>\n",
       "      <td>0.430664</td>\n",
       "      <td>0.961426</td>\n",
       "      <td>0.958008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.409424</td>\n",
       "      <td>-1.884766</td>\n",
       "      <td>-1.027344</td>\n",
       "      <td>1.672852</td>\n",
       "      <td>-1.604492</td>\n",
       "      <td>1.337891</td>\n",
       "      <td>0.055420</td>\n",
       "      <td>0.013466</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069519</td>\n",
       "      <td>1.376953</td>\n",
       "      <td>3.101562</td>\n",
       "      <td>0.869629</td>\n",
       "      <td>1.221680</td>\n",
       "      <td>1.000977</td>\n",
       "      <td>0.544922</td>\n",
       "      <td>0.698730</td>\n",
       "      <td>0.977539</td>\n",
       "      <td>0.828613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.934082</td>\n",
       "      <td>0.628906</td>\n",
       "      <td>0.527344</td>\n",
       "      <td>0.238037</td>\n",
       "      <td>-0.966797</td>\n",
       "      <td>0.547852</td>\n",
       "      <td>-0.059448</td>\n",
       "      <td>-1.707031</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>1.291016</td>\n",
       "      <td>-1.467773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.901855</td>\n",
       "      <td>1.083984</td>\n",
       "      <td>0.979492</td>\n",
       "      <td>0.783203</td>\n",
       "      <td>0.849121</td>\n",
       "      <td>0.894531</td>\n",
       "      <td>0.774902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  lepton pt  lepton eta  lepton phi  missing energy magnitude  \\\n",
       "3    0.0   1.344727   -0.876465    0.936035                  1.992188   \n",
       "4    1.0   1.105469    0.321289    1.522461                  0.882812   \n",
       "5    0.0   1.595703   -0.607910    0.007076                  1.818359   \n",
       "6    1.0   0.409424   -1.884766   -1.027344                  1.672852   \n",
       "7    1.0   0.934082    0.628906    0.527344                  0.238037   \n",
       "\n",
       "   missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag  ...  \\\n",
       "3            0.882324  1.786133  -1.646484  -0.942383     0.000000  ...   \n",
       "4           -1.205078  0.681641  -1.070312  -0.921875     0.000000  ...   \n",
       "5           -0.111877  0.847656  -0.566406   1.581055     2.173828  ...   \n",
       "6           -1.604492  1.337891   0.055420   0.013466     2.173828  ...   \n",
       "7           -0.966797  0.547852  -0.059448  -1.707031     2.173828  ...   \n",
       "\n",
       "   jet 4 eta  jet 4 phi  jet 4 b-tag      m_jj     m_jjj      m_lv     m_jlv  \\\n",
       "3  -0.678223  -1.360352     0.000000  0.946777  1.028320  0.998535  0.728516   \n",
       "4  -0.373535   0.113037     0.000000  0.755859  1.361328  0.986816  0.837891   \n",
       "5  -0.654297  -1.274414     3.101562  0.823730  0.937988  0.971680  0.789062   \n",
       "6   0.069519   1.376953     3.101562  0.869629  1.221680  1.000977  0.544922   \n",
       "7   1.291016  -1.467773     0.000000  0.901855  1.083984  0.979492  0.783203   \n",
       "\n",
       "       m_bb     m_wbb    m_wwbb  \n",
       "3  0.869141  1.026367  0.958008  \n",
       "4  1.132812  0.872070  0.808594  \n",
       "5  0.430664  0.961426  0.958008  \n",
       "6  0.698730  0.977539  0.828613  \n",
       "7  0.849121  0.894531  0.774902  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>lepton pt</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10999995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.160156</td>\n",
       "      <td>1.013672</td>\n",
       "      <td>0.108643</td>\n",
       "      <td>1.495117</td>\n",
       "      <td>-0.537598</td>\n",
       "      <td>2.341797</td>\n",
       "      <td>-0.839844</td>\n",
       "      <td>1.320312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097046</td>\n",
       "      <td>1.190430</td>\n",
       "      <td>3.101562</td>\n",
       "      <td>0.822266</td>\n",
       "      <td>0.766602</td>\n",
       "      <td>1.001953</td>\n",
       "      <td>1.061523</td>\n",
       "      <td>0.836914</td>\n",
       "      <td>0.860352</td>\n",
       "      <td>0.772461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618164</td>\n",
       "      <td>-1.012695</td>\n",
       "      <td>1.110352</td>\n",
       "      <td>0.940918</td>\n",
       "      <td>-0.379150</td>\n",
       "      <td>1.004883</td>\n",
       "      <td>0.348633</td>\n",
       "      <td>-1.678711</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217041</td>\n",
       "      <td>1.048828</td>\n",
       "      <td>3.101562</td>\n",
       "      <td>0.826660</td>\n",
       "      <td>0.989746</td>\n",
       "      <td>1.029297</td>\n",
       "      <td>1.199219</td>\n",
       "      <td>0.891602</td>\n",
       "      <td>0.938477</td>\n",
       "      <td>0.865234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700684</td>\n",
       "      <td>0.774414</td>\n",
       "      <td>1.520508</td>\n",
       "      <td>0.847168</td>\n",
       "      <td>0.211182</td>\n",
       "      <td>1.095703</td>\n",
       "      <td>0.052460</td>\n",
       "      <td>0.024551</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>1.584961</td>\n",
       "      <td>1.713867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337402</td>\n",
       "      <td>0.845215</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.883301</td>\n",
       "      <td>1.888672</td>\n",
       "      <td>1.153320</td>\n",
       "      <td>0.931152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.177734</td>\n",
       "      <td>0.117798</td>\n",
       "      <td>-1.277344</td>\n",
       "      <td>1.864258</td>\n",
       "      <td>-0.584473</td>\n",
       "      <td>0.998535</td>\n",
       "      <td>-1.264648</td>\n",
       "      <td>1.276367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.399414</td>\n",
       "      <td>-1.313477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838867</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>1.201172</td>\n",
       "      <td>0.939453</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>0.759277</td>\n",
       "      <td>0.719238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464355</td>\n",
       "      <td>-0.337158</td>\n",
       "      <td>0.229004</td>\n",
       "      <td>0.954590</td>\n",
       "      <td>-0.868652</td>\n",
       "      <td>0.429932</td>\n",
       "      <td>-0.271240</td>\n",
       "      <td>-1.251953</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.652344</td>\n",
       "      <td>-0.586426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.752441</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.986816</td>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.576172</td>\n",
       "      <td>0.541504</td>\n",
       "      <td>0.517578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label  lepton pt  lepton eta  lepton phi  missing energy magnitude  \\\n",
       "10999995    1.0   1.160156    1.013672    0.108643                  1.495117   \n",
       "10999996    1.0   0.618164   -1.012695    1.110352                  0.940918   \n",
       "10999997    1.0   0.700684    0.774414    1.520508                  0.847168   \n",
       "10999998    0.0   1.177734    0.117798   -1.277344                  1.864258   \n",
       "10999999    0.0   0.464355   -0.337158    0.229004                  0.954590   \n",
       "\n",
       "          missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag  \\\n",
       "10999995           -0.537598  2.341797  -0.839844   1.320312     0.000000   \n",
       "10999996           -0.379150  1.004883   0.348633  -1.678711     2.173828   \n",
       "10999997            0.211182  1.095703   0.052460   0.024551     2.173828   \n",
       "10999998           -0.584473  0.998535  -1.264648   1.276367     0.000000   \n",
       "10999999           -0.868652  0.429932  -0.271240  -1.251953     2.173828   \n",
       "\n",
       "          ...  jet 4 eta  jet 4 phi  jet 4 b-tag      m_jj     m_jjj  \\\n",
       "10999995  ...  -0.097046   1.190430     3.101562  0.822266  0.766602   \n",
       "10999996  ...  -0.217041   1.048828     3.101562  0.826660  0.989746   \n",
       "10999997  ...   1.584961   1.713867     0.000000  0.337402  0.845215   \n",
       "10999998  ...   1.399414  -1.313477     0.000000  0.838867  0.882812   \n",
       "10999999  ...  -1.652344  -0.586426     0.000000  0.752441  0.740723   \n",
       "\n",
       "              m_lv     m_jlv      m_bb     m_wbb    m_wwbb  \n",
       "10999995  1.001953  1.061523  0.836914  0.860352  0.772461  \n",
       "10999996  1.029297  1.199219  0.891602  0.938477  0.865234  \n",
       "10999997  0.987793  0.883301  1.888672  1.153320  0.931152  \n",
       "10999998  1.201172  0.939453  0.339600  0.759277  0.719238  \n",
       "10999999  0.986816  0.664062  0.576172  0.541504  0.517578  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.sample(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = np.array(df_data.iloc[:,1:])\n",
    "lables = np.array(df_data.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "np_data_scaled = sc.fit_transform(np_data)\n",
    "# X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>lepton pt</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5280957</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.751953</td>\n",
       "      <td>1.072266</td>\n",
       "      <td>0.698242</td>\n",
       "      <td>2.412109</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>0.873535</td>\n",
       "      <td>-0.300049</td>\n",
       "      <td>-1.739258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.635742</td>\n",
       "      <td>-0.714355</td>\n",
       "      <td>3.101562</td>\n",
       "      <td>0.771973</td>\n",
       "      <td>0.900879</td>\n",
       "      <td>0.991211</td>\n",
       "      <td>0.945801</td>\n",
       "      <td>0.254150</td>\n",
       "      <td>0.788086</td>\n",
       "      <td>1.271484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6425484</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791016</td>\n",
       "      <td>-0.591309</td>\n",
       "      <td>1.035156</td>\n",
       "      <td>0.283691</td>\n",
       "      <td>1.655273</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>-1.007812</td>\n",
       "      <td>-0.475098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609863</td>\n",
       "      <td>-1.119141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.129883</td>\n",
       "      <td>0.977051</td>\n",
       "      <td>0.989258</td>\n",
       "      <td>0.856934</td>\n",
       "      <td>1.106445</td>\n",
       "      <td>0.788574</td>\n",
       "      <td>0.811523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7985223</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.043945</td>\n",
       "      <td>-0.368164</td>\n",
       "      <td>-1.632812</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>-0.138062</td>\n",
       "      <td>0.992676</td>\n",
       "      <td>0.855469</td>\n",
       "      <td>1.506836</td>\n",
       "      <td>1.086914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.478516</td>\n",
       "      <td>1.307617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389404</td>\n",
       "      <td>0.816895</td>\n",
       "      <td>1.268555</td>\n",
       "      <td>1.177734</td>\n",
       "      <td>3.066406</td>\n",
       "      <td>1.479492</td>\n",
       "      <td>1.063477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8295680</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.546875</td>\n",
       "      <td>0.655273</td>\n",
       "      <td>-0.376953</td>\n",
       "      <td>1.246094</td>\n",
       "      <td>-0.901367</td>\n",
       "      <td>0.498535</td>\n",
       "      <td>0.290039</td>\n",
       "      <td>-1.163086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175293</td>\n",
       "      <td>0.941895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.862305</td>\n",
       "      <td>0.846191</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>1.141602</td>\n",
       "      <td>0.931152</td>\n",
       "      <td>0.955078</td>\n",
       "      <td>0.797363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9504624</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.860352</td>\n",
       "      <td>1.753906</td>\n",
       "      <td>-1.088867</td>\n",
       "      <td>0.548340</td>\n",
       "      <td>1.704102</td>\n",
       "      <td>1.735352</td>\n",
       "      <td>0.864258</td>\n",
       "      <td>1.637695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279297</td>\n",
       "      <td>0.920410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.889648</td>\n",
       "      <td>0.933594</td>\n",
       "      <td>0.985840</td>\n",
       "      <td>0.937012</td>\n",
       "      <td>0.893066</td>\n",
       "      <td>0.870605</td>\n",
       "      <td>0.780273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  lepton pt  lepton eta  lepton phi  missing energy magnitude  \\\n",
       "5280957    0.0   1.751953    1.072266    0.698242                  2.412109   \n",
       "6425484    0.0   1.791016   -0.591309    1.035156                  0.283691   \n",
       "7985223    0.0   1.043945   -0.368164   -1.632812                  0.843750   \n",
       "8295680    1.0   1.546875    0.655273   -0.376953                  1.246094   \n",
       "9504624    1.0   0.860352    1.753906   -1.088867                  0.548340   \n",
       "\n",
       "         missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag  ...  \\\n",
       "5280957            0.966797  0.873535  -0.300049  -1.739258     0.000000  ...   \n",
       "6425484            1.655273  0.695801  -1.007812  -0.475098     0.000000  ...   \n",
       "7985223           -0.138062  0.992676   0.855469   1.506836     1.086914  ...   \n",
       "8295680           -0.901367  0.498535   0.290039  -1.163086     0.000000  ...   \n",
       "9504624            1.704102  1.735352   0.864258   1.637695     0.000000  ...   \n",
       "\n",
       "         jet 4 eta  jet 4 phi  jet 4 b-tag      m_jj     m_jjj      m_lv  \\\n",
       "5280957   0.635742  -0.714355     3.101562  0.771973  0.900879  0.991211   \n",
       "6425484   0.609863  -1.119141     0.000000  1.129883  0.977051  0.989258   \n",
       "7985223  -0.478516   1.307617     0.000000  0.389404  0.816895  1.268555   \n",
       "8295680   0.175293   0.941895     0.000000  0.862305  0.846191  0.987793   \n",
       "9504624   0.279297   0.920410     0.000000  0.889648  0.933594  0.985840   \n",
       "\n",
       "            m_jlv      m_bb     m_wbb    m_wwbb  \n",
       "5280957  0.945801  0.254150  0.788086  1.271484  \n",
       "6425484  0.856934  1.106445  0.788574  0.811523  \n",
       "7985223  1.177734  3.066406  1.479492  1.063477  \n",
       "8295680  1.141602  0.931152  0.955078  0.797363  \n",
       "9504624  0.937012  0.893066  0.870605  0.780273  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(np_data_scaled, lables, test_size = 0.30, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.datasets import mnist\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.style.use('dark_background')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.utils import normalize, to_categorical\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "Nadam = optimizers.Nadam(lr = 0.0001)\n",
    "\n",
    "batch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateBatchScheduler(tf.keras.callbacks.Callback):\n",
    "# \"\"\"Scheduler that dynamically sets the learning rate of model.\"\"\"\n",
    "  \n",
    "    def __init__(self, update_freq=None):\n",
    "        self._update_freq = update_freq\n",
    "        self.lr = 0.15\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self._update_freq and batch % self._update_freq != 0:\n",
    "            return\n",
    "        # To avoid divergence, limit the value range.\n",
    "        \n",
    "        if self.lr <= 0.000001:\n",
    "            self.lr = 0.000001\n",
    "        else:    \n",
    "            self.lr = self.lr - 0.0000002\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)\n",
    "#         print(\"lr:\",self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyLearningRateScheduler(LearningRateScheduler):\n",
    "    \n",
    "#     def on_batch_end(self, batch, logs=None):\n",
    "#         print(\"batch\")\n",
    "#         lr = float(K.get_value(self.model.optimizer.lr))\n",
    "#         print(\"lr:\",lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_decay = 0.00001\n",
    "\n",
    "initializer_input = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.1)\n",
    "\n",
    "initializer_output = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.001)\n",
    "\n",
    "initializer_hidden_layers = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.05)\n",
    "\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', min_delta=0.0001 , patience = 4)\n",
    "\n",
    "callbacks = [\n",
    "     LearningRateBatchScheduler(),\n",
    "#     es\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 300)               8700      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 189,601\n",
      "Trainable params: 189,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(tf.keras.Input(shape=(28,)))\n",
    "model.add(Dense(300,activation = 'tanh',kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay),kernel_initializer = initializer_hidden_layers))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(300,activation = 'tanh',kernel_initializer = initializer_hidden_layers ,kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(300,activation = 'tanh', kernel_initializer = initializer_hidden_layers, kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation = 'sigmoid',kernel_initializer = initializer_output))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "\n",
    "model.compile(optimizer = optimizers.SGD(), loss = 'binary_crossentropy', metrics = ['accuracy',tf.keras.metrics.AUC()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700000 samples, validate on 300000 samples\n",
      "Epoch 1/100\n",
      "700000/700000 [==============================] - 12s 17us/sample - loss: 0.6332 - accuracy: 0.6482 - auc_7: 0.6972 - val_loss: 0.6187 - val_accuracy: 0.6661 - val_auc_7: 0.7203\n",
      "Epoch 2/100\n",
      "700000/700000 [==============================] - 11s 15us/sample - loss: 0.5769 - accuracy: 0.6990 - auc_7: 0.7689 - val_loss: 0.5686 - val_accuracy: 0.7029 - val_auc_7: 0.7785\n",
      "Epoch 3/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5547 - accuracy: 0.7150 - auc_7: 0.7905 - val_loss: 0.5547 - val_accuracy: 0.7137 - val_auc_7: 0.7923\n",
      "Epoch 4/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5438 - accuracy: 0.7225 - auc_7: 0.8005 - val_loss: 0.5429 - val_accuracy: 0.7227 - val_auc_7: 0.8033\n",
      "Epoch 5/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5361 - accuracy: 0.7289 - auc_7: 0.8074 - val_loss: 0.5371 - val_accuracy: 0.7283 - val_auc_7: 0.8068\n",
      "Epoch 6/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5304 - accuracy: 0.7328 - auc_7: 0.8122 - val_loss: 0.5339 - val_accuracy: 0.7294 - val_auc_7: 0.8095\n",
      "Epoch 7/100\n",
      "700000/700000 [==============================] - 11s 15us/sample - loss: 0.5263 - accuracy: 0.7360 - auc_7: 0.8158 - val_loss: 0.5278 - val_accuracy: 0.7353 - val_auc_7: 0.8147\n",
      "Epoch 8/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5228 - accuracy: 0.7380 - auc_7: 0.8187 - val_loss: 0.5228 - val_accuracy: 0.7382 - val_auc_7: 0.8187\n",
      "Epoch 9/100\n",
      "700000/700000 [==============================] - 11s 15us/sample - loss: 0.5197 - accuracy: 0.7398 - auc_7: 0.8213 - val_loss: 0.5247 - val_accuracy: 0.7366 - val_auc_7: 0.8184\n",
      "Epoch 10/100\n",
      "700000/700000 [==============================] - 11s 15us/sample - loss: 0.5173 - accuracy: 0.7416 - auc_7: 0.8232 - val_loss: 0.5202 - val_accuracy: 0.7399 - val_auc_7: 0.8222\n",
      "Epoch 11/100\n",
      "700000/700000 [==============================] - 11s 15us/sample - loss: 0.5150 - accuracy: 0.7434 - auc_7: 0.8251 - val_loss: 0.5226 - val_accuracy: 0.7383 - val_auc_7: 0.8233\n",
      "Epoch 12/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5132 - accuracy: 0.7447 - auc_7: 0.8267 - val_loss: 0.5192 - val_accuracy: 0.7413 - val_auc_7: 0.8227\n",
      "Epoch 13/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5115 - accuracy: 0.7455 - auc_7: 0.8280 - val_loss: 0.5152 - val_accuracy: 0.7437 - val_auc_7: 0.8256\n",
      "Epoch 14/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5102 - accuracy: 0.7465 - auc_7: 0.8290 - val_loss: 0.5149 - val_accuracy: 0.7439 - val_auc_7: 0.8262\n",
      "Epoch 15/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5090 - accuracy: 0.7474 - auc_7: 0.8301 - val_loss: 0.5142 - val_accuracy: 0.7443 - val_auc_7: 0.8267\n",
      "Epoch 16/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5074 - accuracy: 0.7489 - auc_7: 0.8313 - val_loss: 0.5167 - val_accuracy: 0.7424 - val_auc_7: 0.8249\n",
      "Epoch 17/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.5063 - accuracy: 0.7494 - auc_7: 0.8323 - val_loss: 0.5102 - val_accuracy: 0.7474 - val_auc_7: 0.8295\n",
      "Epoch 18/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5052 - accuracy: 0.7500 - auc_7: 0.8331 - val_loss: 0.5159 - val_accuracy: 0.7435 - val_auc_7: 0.8250\n",
      "Epoch 19/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5040 - accuracy: 0.7509 - auc_7: 0.8342 - val_loss: 0.5125 - val_accuracy: 0.7457 - val_auc_7: 0.8301\n",
      "Epoch 20/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5029 - accuracy: 0.7516 - auc_7: 0.8350 - val_loss: 0.5150 - val_accuracy: 0.7449 - val_auc_7: 0.8271\n",
      "Epoch 21/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5021 - accuracy: 0.7524 - auc_7: 0.8357 - val_loss: 0.5079 - val_accuracy: 0.7489 - val_auc_7: 0.8315\n",
      "Epoch 22/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5010 - accuracy: 0.7529 - auc_7: 0.8366 - val_loss: 0.5197 - val_accuracy: 0.7408 - val_auc_7: 0.8235\n",
      "Epoch 23/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.5000 - accuracy: 0.7538 - auc_7: 0.8374 - val_loss: 0.5083 - val_accuracy: 0.7490 - val_auc_7: 0.8319\n",
      "Epoch 24/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4991 - accuracy: 0.7542 - auc_7: 0.8381 - val_loss: 0.5083 - val_accuracy: 0.7485 - val_auc_7: 0.8316\n",
      "Epoch 25/100\n",
      "700000/700000 [==============================] - 11s 15us/sample - loss: 0.4983 - accuracy: 0.7552 - auc_7: 0.8388 - val_loss: 0.5180 - val_accuracy: 0.7432 - val_auc_7: 0.8325\n",
      "Epoch 26/100\n",
      "700000/700000 [==============================] - 11s 15us/sample - loss: 0.4974 - accuracy: 0.7560 - auc_7: 0.8395 - val_loss: 0.5067 - val_accuracy: 0.7501 - val_auc_7: 0.8325\n",
      "Epoch 27/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4964 - accuracy: 0.7564 - auc_7: 0.8403 - val_loss: 0.5104 - val_accuracy: 0.7478 - val_auc_7: 0.8301\n",
      "Epoch 28/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4956 - accuracy: 0.7571 - auc_7: 0.8410 - val_loss: 0.5122 - val_accuracy: 0.7464 - val_auc_7: 0.8290\n",
      "Epoch 29/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4949 - accuracy: 0.7580 - auc_7: 0.8415 - val_loss: 0.5138 - val_accuracy: 0.7459 - val_auc_7: 0.8325\n",
      "Epoch 30/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4942 - accuracy: 0.7585 - auc_7: 0.8422 - val_loss: 0.5062 - val_accuracy: 0.7515 - val_auc_7: 0.8342\n",
      "Epoch 31/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4932 - accuracy: 0.7587 - auc_7: 0.8429 - val_loss: 0.5034 - val_accuracy: 0.7519 - val_auc_7: 0.8357\n",
      "Epoch 32/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4927 - accuracy: 0.7594 - auc_7: 0.8434 - val_loss: 0.5053 - val_accuracy: 0.7520 - val_auc_7: 0.8352\n",
      "Epoch 33/100\n",
      "700000/700000 [==============================] - 11s 15us/sample - loss: 0.4918 - accuracy: 0.7601 - auc_7: 0.8441 - val_loss: 0.5052 - val_accuracy: 0.7515 - val_auc_7: 0.8343\n",
      "Epoch 34/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4910 - accuracy: 0.7609 - auc_7: 0.8447 - val_loss: 0.5095 - val_accuracy: 0.7490 - val_auc_7: 0.8345\n",
      "Epoch 35/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4903 - accuracy: 0.7614 - auc_7: 0.8453 - val_loss: 0.5056 - val_accuracy: 0.7512 - val_auc_7: 0.8341\n",
      "Epoch 36/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4896 - accuracy: 0.7615 - auc_7: 0.8458 - val_loss: 0.5063 - val_accuracy: 0.7513 - val_auc_7: 0.8344\n",
      "Epoch 37/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4888 - accuracy: 0.7623 - auc_7: 0.8465 - val_loss: 0.5052 - val_accuracy: 0.7518 - val_auc_7: 0.8354\n",
      "Epoch 38/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4880 - accuracy: 0.7631 - auc_7: 0.8472 - val_loss: 0.5050 - val_accuracy: 0.7520 - val_auc_7: 0.8353\n",
      "Epoch 39/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4875 - accuracy: 0.7632 - auc_7: 0.8476 - val_loss: 0.5086 - val_accuracy: 0.7495 - val_auc_7: 0.8356\n",
      "Epoch 40/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4868 - accuracy: 0.7640 - auc_7: 0.8481 - val_loss: 0.5059 - val_accuracy: 0.7513 - val_auc_7: 0.8350\n",
      "Epoch 41/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4861 - accuracy: 0.7644 - auc_7: 0.8487 - val_loss: 0.5069 - val_accuracy: 0.7511 - val_auc_7: 0.8366\n",
      "Epoch 42/100\n",
      "700000/700000 [==============================] - 10s 15us/sample - loss: 0.4855 - accuracy: 0.7650 - auc_7: 0.8493 - val_loss: 0.5070 - val_accuracy: 0.7510 - val_auc_7: 0.8348\n",
      "Epoch 43/100\n",
      "700000/700000 [==============================] - 11s 15us/sample - loss: 0.4848 - accuracy: 0.7659 - auc_7: 0.8498 - val_loss: 0.5054 - val_accuracy: 0.7519 - val_auc_7: 0.8350\n",
      "Epoch 44/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4846 - accuracy: 0.7657 - auc_7: 0.8500 - val_loss: 0.5039 - val_accuracy: 0.7532 - val_auc_7: 0.8368\n",
      "Epoch 45/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4836 - accuracy: 0.7663 - auc_7: 0.8508 - val_loss: 0.5052 - val_accuracy: 0.7532 - val_auc_7: 0.8358\n",
      "Epoch 46/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4830 - accuracy: 0.7670 - auc_7: 0.8513 - val_loss: 0.5080 - val_accuracy: 0.7506 - val_auc_7: 0.8342\n",
      "Epoch 47/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4823 - accuracy: 0.7672 - auc_7: 0.8518 - val_loss: 0.5054 - val_accuracy: 0.7524 - val_auc_7: 0.8354\n",
      "Epoch 48/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4817 - accuracy: 0.7678 - auc_7: 0.8524 - val_loss: 0.5076 - val_accuracy: 0.7512 - val_auc_7: 0.8344\n",
      "Epoch 49/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4810 - accuracy: 0.7679 - auc_7: 0.8528 - val_loss: 0.5097 - val_accuracy: 0.7501 - val_auc_7: 0.8338\n",
      "Epoch 50/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4805 - accuracy: 0.7687 - auc_7: 0.8533 - val_loss: 0.5078 - val_accuracy: 0.7518 - val_auc_7: 0.8350\n",
      "Epoch 51/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4799 - accuracy: 0.7693 - auc_7: 0.8538 - val_loss: 0.5063 - val_accuracy: 0.7526 - val_auc_7: 0.8354\n",
      "Epoch 52/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4792 - accuracy: 0.7701 - auc_7: 0.8544 - val_loss: 0.5103 - val_accuracy: 0.7503 - val_auc_7: 0.8331\n",
      "Epoch 53/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4788 - accuracy: 0.7698 - auc_7: 0.8547 - val_loss: 0.5090 - val_accuracy: 0.7508 - val_auc_7: 0.8339\n",
      "Epoch 54/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4782 - accuracy: 0.7704 - auc_7: 0.8552 - val_loss: 0.5074 - val_accuracy: 0.7512 - val_auc_7: 0.8354\n",
      "Epoch 55/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4773 - accuracy: 0.7710 - auc_7: 0.8559 - val_loss: 0.5095 - val_accuracy: 0.7509 - val_auc_7: 0.8334\n",
      "Epoch 56/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4768 - accuracy: 0.7716 - auc_7: 0.8563 - val_loss: 0.5114 - val_accuracy: 0.7503 - val_auc_7: 0.8345\n",
      "Epoch 57/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4763 - accuracy: 0.7714 - auc_7: 0.8568 - val_loss: 0.5104 - val_accuracy: 0.7509 - val_auc_7: 0.8333\n",
      "Epoch 58/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4756 - accuracy: 0.7724 - auc_7: 0.8574 - val_loss: 0.5087 - val_accuracy: 0.7533 - val_auc_7: 0.8354\n",
      "Epoch 59/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4751 - accuracy: 0.7728 - auc_7: 0.8578 - val_loss: 0.5092 - val_accuracy: 0.7518 - val_auc_7: 0.8348\n",
      "Epoch 60/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4745 - accuracy: 0.7730 - auc_7: 0.8583 - val_loss: 0.5097 - val_accuracy: 0.7516 - val_auc_7: 0.8341\n",
      "Epoch 61/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4738 - accuracy: 0.7741 - auc_7: 0.8589 - val_loss: 0.5136 - val_accuracy: 0.7492 - val_auc_7: 0.8332\n",
      "Epoch 62/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4733 - accuracy: 0.7740 - auc_7: 0.8592 - val_loss: 0.5109 - val_accuracy: 0.7517 - val_auc_7: 0.8342\n",
      "Epoch 63/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4728 - accuracy: 0.7747 - auc_7: 0.8597 - val_loss: 0.5114 - val_accuracy: 0.7509 - val_auc_7: 0.8338\n",
      "Epoch 64/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4722 - accuracy: 0.7751 - auc_7: 0.8601 - val_loss: 0.5120 - val_accuracy: 0.7497 - val_auc_7: 0.8340\n",
      "Epoch 65/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4715 - accuracy: 0.7753 - auc_7: 0.8607 - val_loss: 0.5116 - val_accuracy: 0.7518 - val_auc_7: 0.8346\n",
      "Epoch 66/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4709 - accuracy: 0.7762 - auc_7: 0.8612 - val_loss: 0.5159 - val_accuracy: 0.7490 - val_auc_7: 0.8325\n",
      "Epoch 67/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4705 - accuracy: 0.7769 - auc_7: 0.8616 - val_loss: 0.5181 - val_accuracy: 0.7489 - val_auc_7: 0.8326\n",
      "Epoch 68/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4697 - accuracy: 0.7771 - auc_7: 0.8622 - val_loss: 0.5210 - val_accuracy: 0.7446 - val_auc_7: 0.8297\n",
      "Epoch 69/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4691 - accuracy: 0.7772 - auc_7: 0.8626 - val_loss: 0.5198 - val_accuracy: 0.7462 - val_auc_7: 0.8317\n",
      "Epoch 70/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4684 - accuracy: 0.7780 - auc_7: 0.8631 - val_loss: 0.5173 - val_accuracy: 0.7504 - val_auc_7: 0.8333\n",
      "Epoch 71/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4681 - accuracy: 0.7783 - auc_7: 0.8635 - val_loss: 0.5140 - val_accuracy: 0.7501 - val_auc_7: 0.8325\n",
      "Epoch 72/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4674 - accuracy: 0.7785 - auc_7: 0.8639 - val_loss: 0.5150 - val_accuracy: 0.7510 - val_auc_7: 0.8339\n",
      "Epoch 73/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4667 - accuracy: 0.7791 - auc_7: 0.8646 - val_loss: 0.5191 - val_accuracy: 0.7488 - val_auc_7: 0.8313\n",
      "Epoch 74/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4662 - accuracy: 0.7796 - auc_7: 0.8650 - val_loss: 0.5156 - val_accuracy: 0.7497 - val_auc_7: 0.8322\n",
      "Epoch 75/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4655 - accuracy: 0.7801 - auc_7: 0.8655 - val_loss: 0.5273 - val_accuracy: 0.7436 - val_auc_7: 0.8293\n",
      "Epoch 76/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4650 - accuracy: 0.7805 - auc_7: 0.8659 - val_loss: 0.5211 - val_accuracy: 0.7476 - val_auc_7: 0.8309\n",
      "Epoch 77/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4643 - accuracy: 0.7810 - auc_7: 0.8665 - val_loss: 0.5230 - val_accuracy: 0.7471 - val_auc_7: 0.8301\n",
      "Epoch 78/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4637 - accuracy: 0.7816 - auc_7: 0.8670 - val_loss: 0.5218 - val_accuracy: 0.7462 - val_auc_7: 0.8293\n",
      "Epoch 79/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4632 - accuracy: 0.7824 - auc_7: 0.8675 - val_loss: 0.5244 - val_accuracy: 0.7471 - val_auc_7: 0.8305\n",
      "Epoch 80/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4626 - accuracy: 0.7828 - auc_7: 0.8679 - val_loss: 0.5199 - val_accuracy: 0.7501 - val_auc_7: 0.8322\n",
      "Epoch 81/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4617 - accuracy: 0.7829 - auc_7: 0.8686 - val_loss: 0.5230 - val_accuracy: 0.7484 - val_auc_7: 0.8306\n",
      "Epoch 82/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4612 - accuracy: 0.7838 - auc_7: 0.8690 - val_loss: 0.5207 - val_accuracy: 0.7490 - val_auc_7: 0.8312\n",
      "Epoch 83/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4608 - accuracy: 0.7837 - auc_7: 0.8693 - val_loss: 0.5254 - val_accuracy: 0.7450 - val_auc_7: 0.8282\n",
      "Epoch 84/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4601 - accuracy: 0.7846 - auc_7: 0.8699 - val_loss: 0.5287 - val_accuracy: 0.7445 - val_auc_7: 0.8267\n",
      "Epoch 85/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4594 - accuracy: 0.7849 - auc_7: 0.8704 - val_loss: 0.5301 - val_accuracy: 0.7460 - val_auc_7: 0.8277\n",
      "Epoch 86/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4587 - accuracy: 0.7857 - auc_7: 0.8709 - val_loss: 0.5272 - val_accuracy: 0.7461 - val_auc_7: 0.8278\n",
      "Epoch 87/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4582 - accuracy: 0.7861 - auc_7: 0.8714 - val_loss: 0.5250 - val_accuracy: 0.7480 - val_auc_7: 0.8297\n",
      "Epoch 88/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4576 - accuracy: 0.7862 - auc_7: 0.8718 - val_loss: 0.5267 - val_accuracy: 0.7469 - val_auc_7: 0.8289\n",
      "Epoch 89/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4567 - accuracy: 0.7872 - auc_7: 0.8725 - val_loss: 0.5259 - val_accuracy: 0.7469 - val_auc_7: 0.8294\n",
      "Epoch 90/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4562 - accuracy: 0.7875 - auc_7: 0.8730 - val_loss: 0.5283 - val_accuracy: 0.7474 - val_auc_7: 0.8293\n",
      "Epoch 91/100\n",
      "700000/700000 [==============================] - 11s 15us/sample - loss: 0.4554 - accuracy: 0.7880 - auc_7: 0.8735 - val_loss: 0.5284 - val_accuracy: 0.7475 - val_auc_7: 0.8289\n",
      "Epoch 92/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4551 - accuracy: 0.7885 - auc_7: 0.8738 - val_loss: 0.5301 - val_accuracy: 0.7444 - val_auc_7: 0.8270\n",
      "Epoch 93/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4542 - accuracy: 0.7891 - auc_7: 0.8745 - val_loss: 0.5324 - val_accuracy: 0.7462 - val_auc_7: 0.8285\n",
      "Epoch 94/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4537 - accuracy: 0.7896 - auc_7: 0.8749 - val_loss: 0.5320 - val_accuracy: 0.7451 - val_auc_7: 0.8267\n",
      "Epoch 95/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4529 - accuracy: 0.7904 - auc_7: 0.8755 - val_loss: 0.5328 - val_accuracy: 0.7439 - val_auc_7: 0.8276\n",
      "Epoch 96/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4524 - accuracy: 0.7901 - auc_7: 0.8759 - val_loss: 0.5363 - val_accuracy: 0.7435 - val_auc_7: 0.8255\n",
      "Epoch 97/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4517 - accuracy: 0.7914 - auc_7: 0.8764 - val_loss: 0.5387 - val_accuracy: 0.7444 - val_auc_7: 0.8268\n",
      "Epoch 98/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4512 - accuracy: 0.7912 - auc_7: 0.8769 - val_loss: 0.5338 - val_accuracy: 0.7456 - val_auc_7: 0.8274\n",
      "Epoch 99/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4504 - accuracy: 0.7919 - auc_7: 0.8774 - val_loss: 0.5376 - val_accuracy: 0.7442 - val_auc_7: 0.8257\n",
      "Epoch 100/100\n",
      "700000/700000 [==============================] - 11s 16us/sample - loss: 0.4499 - accuracy: 0.7926 - auc_7: 0.8779 - val_loss: 0.5354 - val_accuracy: 0.7436 - val_auc_7: 0.8252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26c42b20988>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "model.fit(x_train_split, y_train_split, batch_size = 256, validation_data=(x_test_split,y_test_split), epochs = 100,callbacks = callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-Capstone] *",
   "language": "python",
   "name": "conda-env-.conda-Capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
