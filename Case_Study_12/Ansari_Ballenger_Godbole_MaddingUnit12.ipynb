{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 12: Case Study \n",
    "\n",
    "Allen Ansari, Chris Ballenger, Shantanu Godbole, Chad Madding\n",
    "\n",
    "DS 7333 Quantifying the World\n",
    "\n",
    "July 25, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following paper: https://arxiv.org/pdf/1402.4735.pdf \n",
    "\n",
    "Build a replica Neural Network with the paper’s architecture using Tensorflow. \n",
    "\n",
    "If possible, begin to train on the data located here: https://archive.ics.uci.edu/ml/datasets/HIGGS \n",
    "\n",
    "How close can you get to the original results? \n",
    "\n",
    "To facilitate quicker training, you may increase the batch size temporarily (this has a small impact on the final result but can speed your calculations significantly). You do not need to train a final result using the paper’s parameters; only the code for your model is required in your final submission. \n",
    "\n",
    "Include in your report: \n",
    "Based on the class notes and discussion, suggest improvements to the procedure. \n",
    "What are standard practices now versus when this paper was written? \n",
    "What kind of improvements do they provide? \n",
    "How would you quantify if your result duplicated the papers? \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Abstract & Introduction](#Abstract)\n",
    "2. [Data](#Data)\n",
    "3. [Description of Neural Network in the Paper](#Description-of-Neural-Network-in-the-Paper)\n",
    "4. [Re-Creation of Neural Network in Keras](#Re-Creation-of-Neural-Network-in-Keras)\n",
    " - [Observations and Workarounds](#Observations-and-Workarounds) \n",
    " - [Neural Network Code](#Neural-Network-Code) \n",
    "5. [Enhancements to the Network](#Enhancements-to-the-Network) \n",
    "6. [Conclusion](#Conclusion)\n",
    "7. [Appendix](#Appendix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract \n",
    "\n",
    "We will be carrying out a classification problem to distinguish between a signal process that produces Higgs bosons and a background process that does not. Our information is based on outdated code, so in the process, we will reproduce results with more updated information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are standard practices now versus when this paper was written? \n",
    "\n",
    "In the paper “Searching for Exotic Particles in High-Energy Physics with Deep Learning,” published in 2014, the authors looked at using deep learning models to assist in classifying “exotic particle(s)” that result from particle collisions. Their paper focused on using the Higgs Boson data collected from the Large Hadron Collider’s detectors, and their study focused on improving past research that used ‘shallow’ machine learning models. According to the paper, these models have a limited capacity to learn complex non-linear functions. \n",
    "\n",
    "Their findings ended with Deep Learning outperforming the standard at the time, “shallow” models. The original research and building of the model used libraries standard in 2014. The original code, implemented through pylearn2, is no longer supported. \n",
    "\n",
    "In this project, we will use the same dataset and the Keras Python library with a TensorFlow backend to replicate their original work using more modern approaches. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras \n",
    "\n",
    "Keras is a neural network library written in Python and can run on top of TensorFlow and other existing toolkits. It is touted for its’ ease of use and can be up and running with just a few lines of code while still being customizable enough for layer by layer programming. \n",
    "\n",
    "### TensorFlow \n",
    "\n",
    "TensorFlow is a free and open-source platform developed by the Google Brain Team. It is a software library for dataflow and differentiable programming across a range of tasks and used for machine learning applications such as neural networks. (TensorFlow, n.d.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description-of-Neural-Network-in-the-Paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Five-layered neural network with 300 hidden units in each layer\n",
    "\n",
    "* The Activation function that was used in all layers and all nodes is 'tanh'\n",
    "\n",
    "* Weights were initialized from a normal distribution with zero mean and standard deviation 0.1 in the first layer, 0.001 in the output layer, and 0.05 all other hidden layers\n",
    "\n",
    "* Weight decay coefficient of 1 × 10^-5 was set for all the layers\n",
    "\n",
    "* Stocastic Gradient Descent was used as the optimizer\n",
    "\n",
    "* The initial learning rate was set to 0.05 and thereafter the learning rate was decayed by a factor of 1.0000002 every batch until it reached to the minimum learning rate of 10^-6\n",
    "\n",
    "* A momentum term increased linearly over the first 200 epochs from 0.9 to 0.99, at which point it remained constant\n",
    "\n",
    "* Gradient computations were made on mini-batches of size 100.\n",
    "\n",
    "* Training ended when the momentum had reached its maximum value and the minimum error on the validation set (500,000 examples) had not decreased by more than a factor of 0.00001 over 10 epochs.\n",
    "\n",
    "* The Validation size was 500,000\n",
    "\n",
    "* The Neural Network was created using the Theano library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Back to Top](#Table-of-Contents)\n",
    "____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Creation-of-Neural-Network-in-Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations-and-Workarounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. There was no option that was found to dynamically adjust the momentum in keras, therefore the standard SGD momentum was used \n",
    " \n",
    " 2. Also, as a result of no momentum change, the early stopping condition that involves momentum could not be used\n",
    " \n",
    " 3. To replicate the lr decay, the 'LR Scdeduler' that is provided by Keras callback functions was earlier used but that decays the learning rate per Epoch which did not fit our requirement\n",
    " \n",
    " 4. To adress the above problem, a class `LearningRateBatchScheduler` was inherited from `(tf.keras.callbacks.Callback)`. The on_batch_end method was used to make sure that the change in Learning Rate happens at the end of the batch. The print statement in the code (which is now commented) was used to verify the same. We referred to the [Stackoverflow Link](https://stackoverflow.com/questions/52277003/how-to-implement-exponentially-decay-learning-rate-in-keras-by-following-the-glo) for this.\n",
    " \n",
    " 5. The Weight Decay were set by using l2 function in the kernel regularizer \n",
    " \n",
    " 6. For setting the initial weights in the different laters, per the normal discribution and different standard deviations mentioned in the paper, `kernel_initializer` parameter in the dense layer was used   \n",
    " \n",
    " 7. We used the Validation set that is 30% of the total dataset, just to speed up the training by reducing the number of records in the test set\n",
    " \n",
    " 8. Batch size in the paper was 100, but again for speeding up the training we will be using bath size of 500\n",
    " \n",
    " 9. The minimum change in loss for early stopping, was increased to '0.0001' , from '0.00001' and the patience was decreased to 2 for stopping the training earlier as the Network was still getting decent AUC \n",
    " \n",
    " 10. As per [this Medium article](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1#:~:text=The%20mathematical%20form%20of%20time,t%20is%20the%20iteration%20number.) the formula for Exponential lr decay is `lr = lr0 * e^(−kt)` where lr, k are hyperparameters and t is the iteration number. We tried 1.0000002 as the rate that was mentioned in the paper. The LR was being floored to minimum in just 3 batches. To facilitate gradual decrease of the Learning rate we used 0.0000002 as the decay rate. However the use of class `LearningRateBatchScheduler_exp` that implements this formula made the AUC to be stuck around 0.75. We tried linear decay instead of exponential decay that is implemented in the class `LearningRateBatchScheduler_linear`, which gave us the AUC that is comparable to what was acchieved in the paper\n",
    " \n",
    " 11. The Paper mentions the use of training layers using Autoencoder and then using the layers tarined in the autoencoder (pre-trained layers) in the classification Neural Netowrk. This can be potentially tried as a further imporvement to the basic NN with Dropout.\n",
    " \n",
    " 12. We have scaled to a mean of 0 and standard diev of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural-Network-Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the data \n",
    "\n",
    "df_data = pd.read_hdf('data.h5', 'table', where=['index>2'])\n",
    "\n",
    "np_data = np.array(df_data.iloc[:,1:])\n",
    "lables = np.array(df_data.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>lepton pt</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10999995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.160156</td>\n",
       "      <td>1.013672</td>\n",
       "      <td>0.108643</td>\n",
       "      <td>1.495117</td>\n",
       "      <td>-0.537598</td>\n",
       "      <td>2.341797</td>\n",
       "      <td>-0.839844</td>\n",
       "      <td>1.320312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097046</td>\n",
       "      <td>1.190430</td>\n",
       "      <td>3.101562</td>\n",
       "      <td>0.822266</td>\n",
       "      <td>0.766602</td>\n",
       "      <td>1.001953</td>\n",
       "      <td>1.061523</td>\n",
       "      <td>0.836914</td>\n",
       "      <td>0.860352</td>\n",
       "      <td>0.772461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618164</td>\n",
       "      <td>-1.012695</td>\n",
       "      <td>1.110352</td>\n",
       "      <td>0.940918</td>\n",
       "      <td>-0.379150</td>\n",
       "      <td>1.004883</td>\n",
       "      <td>0.348633</td>\n",
       "      <td>-1.678711</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217041</td>\n",
       "      <td>1.048828</td>\n",
       "      <td>3.101562</td>\n",
       "      <td>0.826660</td>\n",
       "      <td>0.989746</td>\n",
       "      <td>1.029297</td>\n",
       "      <td>1.199219</td>\n",
       "      <td>0.891602</td>\n",
       "      <td>0.938477</td>\n",
       "      <td>0.865234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700684</td>\n",
       "      <td>0.774414</td>\n",
       "      <td>1.520508</td>\n",
       "      <td>0.847168</td>\n",
       "      <td>0.211182</td>\n",
       "      <td>1.095703</td>\n",
       "      <td>0.052460</td>\n",
       "      <td>0.024551</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>1.584961</td>\n",
       "      <td>1.713867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337402</td>\n",
       "      <td>0.845215</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.883301</td>\n",
       "      <td>1.888672</td>\n",
       "      <td>1.153320</td>\n",
       "      <td>0.931152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.177734</td>\n",
       "      <td>0.117798</td>\n",
       "      <td>-1.277344</td>\n",
       "      <td>1.864258</td>\n",
       "      <td>-0.584473</td>\n",
       "      <td>0.998535</td>\n",
       "      <td>-1.264648</td>\n",
       "      <td>1.276367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.399414</td>\n",
       "      <td>-1.313477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838867</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>1.201172</td>\n",
       "      <td>0.939453</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>0.759277</td>\n",
       "      <td>0.719238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464355</td>\n",
       "      <td>-0.337158</td>\n",
       "      <td>0.229004</td>\n",
       "      <td>0.954590</td>\n",
       "      <td>-0.868652</td>\n",
       "      <td>0.429932</td>\n",
       "      <td>-0.271240</td>\n",
       "      <td>-1.251953</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.652344</td>\n",
       "      <td>-0.586426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.752441</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.986816</td>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.576172</td>\n",
       "      <td>0.541504</td>\n",
       "      <td>0.517578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label  lepton pt  lepton eta  lepton phi  missing energy magnitude  \\\n",
       "10999995    1.0   1.160156    1.013672    0.108643                  1.495117   \n",
       "10999996    1.0   0.618164   -1.012695    1.110352                  0.940918   \n",
       "10999997    1.0   0.700684    0.774414    1.520508                  0.847168   \n",
       "10999998    0.0   1.177734    0.117798   -1.277344                  1.864258   \n",
       "10999999    0.0   0.464355   -0.337158    0.229004                  0.954590   \n",
       "\n",
       "          missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag  \\\n",
       "10999995           -0.537598  2.341797  -0.839844   1.320312     0.000000   \n",
       "10999996           -0.379150  1.004883   0.348633  -1.678711     2.173828   \n",
       "10999997            0.211182  1.095703   0.052460   0.024551     2.173828   \n",
       "10999998           -0.584473  0.998535  -1.264648   1.276367     0.000000   \n",
       "10999999           -0.868652  0.429932  -0.271240  -1.251953     2.173828   \n",
       "\n",
       "          ...  jet 4 eta  jet 4 phi  jet 4 b-tag      m_jj     m_jjj  \\\n",
       "10999995  ...  -0.097046   1.190430     3.101562  0.822266  0.766602   \n",
       "10999996  ...  -0.217041   1.048828     3.101562  0.826660  0.989746   \n",
       "10999997  ...   1.584961   1.713867     0.000000  0.337402  0.845215   \n",
       "10999998  ...   1.399414  -1.313477     0.000000  0.838867  0.882812   \n",
       "10999999  ...  -1.652344  -0.586426     0.000000  0.752441  0.740723   \n",
       "\n",
       "              m_lv     m_jlv      m_bb     m_wbb    m_wwbb  \n",
       "10999995  1.001953  1.061523  0.836914  0.860352  0.772461  \n",
       "10999996  1.029297  1.199219  0.891602  0.938477  0.865234  \n",
       "10999997  0.987793  0.883301  1.888672  1.153320  0.931152  \n",
       "10999998  1.201172  0.939453  0.339600  0.759277  0.719238  \n",
       "10999999  0.986816  0.664062  0.576172  0.541504  0.517578  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "np_data_scaled = sc.fit_transform(np_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(np_data_scaled, lables, test_size = 0.30, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.utils import normalize, to_categorical\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nueral Network Parameters\n",
    "\n",
    "batch_size = 500 #Batch Size modified from what was mentioned in the paper to speed up the training process\n",
    "weights_decay = 0.00001\n",
    "\n",
    "lr = 0.15\n",
    "min_lr = 0.000001\n",
    "lr_decay_factor = 0.0000002\n",
    "lr_decay_factor_e = 0.0000002\n",
    "\n",
    "\n",
    "initializer_input = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.1)\n",
    "initializer_output = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.001)\n",
    "initializer_hidden_layers = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateBatchScheduler_linear(tf.keras.callbacks.Callback):\n",
    "  \n",
    "    def __init__(self, update_freq=None):\n",
    "        self._update_freq = update_freq\n",
    "        self.lr =lr\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self._update_freq and batch % self._update_freq != 0:\n",
    "            return\n",
    "        # To avoid divergence, limit the value range.\n",
    "        \n",
    "        if self.lr <= min_lr:\n",
    "            self.lr = min_lr\n",
    "        else:    \n",
    "            self.lr = self.lr - lr_decay_factor\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)\n",
    "#         print(\"lr:\",self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateBatchScheduler_exp(tf.keras.callbacks.Callback):\n",
    "  \n",
    "    def __init__(self, update_freq=None):\n",
    "        self._update_freq = update_freq\n",
    "        self.lr = lr\n",
    "        self.iteration = 1\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self._update_freq and batch % self._update_freq != 0:\n",
    "            return\n",
    "        # To avoid divergence, limit the value range.\n",
    "        \n",
    "        if self.lr <= min_lr:\n",
    "            self.lr = min_lr\n",
    "        else:    \n",
    "            self.lr = self.lr * np.exp(-lr_decay_factor_e * self.iteration)\n",
    "            self.iteration = self.iteration + 1\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)\n",
    "#         print(\"lr:\",self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Early Stopping \n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='loss', mode='min', min_delta = 0.0001 , patience = 2)\n",
    "\n",
    "callbacks_a = [ LearningRateBatchScheduler_linear(), earlyStopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 300)               8700      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 189,601\n",
      "Trainable params: 189,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(tf.keras.Input(shape=(28,)))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(300,activation = 'tanh',kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay),kernel_initializer = initializer_hidden_layers))\n",
    "\n",
    "model.add(Dense(300,activation = 'tanh',kernel_initializer = initializer_hidden_layers ,kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "\n",
    "model.add(Dense(300,activation = 'tanh', kernel_initializer = initializer_hidden_layers, kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "\n",
    "model.add(Dense(1,activation = 'sigmoid',kernel_initializer = initializer_output))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "model.compile(optimizer = optimizers.SGD(), loss = 'binary_crossentropy', metrics = ['accuracy',tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7699997 samples, validate on 3300000 samples\n",
      "Epoch 1/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.5590 - accuracy: 0.7108 - auc: 0.7855 - val_loss: 0.5246 - val_accuracy: 0.7369 - val_auc: 0.8183\n",
      "Epoch 2/1000\n",
      "7699997/7699997 [==============================] - 58s 8us/sample - loss: 0.5185 - accuracy: 0.7413 - auc: 0.8223 - val_loss: 0.5135 - val_accuracy: 0.7443 - val_auc: 0.8266\n",
      "Epoch 3/1000\n",
      "7699997/7699997 [==============================] - 59s 8us/sample - loss: 0.5101 - accuracy: 0.7470 - auc: 0.8291 - val_loss: 0.5064 - val_accuracy: 0.7493 - val_auc: 0.8322\n",
      "Epoch 4/1000\n",
      "7699997/7699997 [==============================] - 56s 7us/sample - loss: 0.5045 - accuracy: 0.7508 - auc: 0.8335 - val_loss: 0.5023 - val_accuracy: 0.7523 - val_auc: 0.8358\n",
      "Epoch 5/1000\n",
      "7699997/7699997 [==============================] - 56s 7us/sample - loss: 0.5005 - accuracy: 0.7536 - auc: 0.8367 - val_loss: 0.4977 - val_accuracy: 0.7554 - val_auc: 0.8389\n",
      "Epoch 6/1000\n",
      "7699997/7699997 [==============================] - 56s 7us/sample - loss: 0.4974 - accuracy: 0.7558 - auc: 0.8391 - val_loss: 0.4967 - val_accuracy: 0.7563 - val_auc: 0.8406\n",
      "Epoch 7/1000\n",
      "7699997/7699997 [==============================] - 57s 7us/sample - loss: 0.4948 - accuracy: 0.7576 - auc: 0.8411 - val_loss: 0.4935 - val_accuracy: 0.7585 - val_auc: 0.8423\n",
      "Epoch 8/1000\n",
      "7699997/7699997 [==============================] - 56s 7us/sample - loss: 0.4924 - accuracy: 0.7594 - auc: 0.8429 - val_loss: 0.4927 - val_accuracy: 0.7589 - val_auc: 0.8438\n",
      "Epoch 9/1000\n",
      "7699997/7699997 [==============================] - 57s 7us/sample - loss: 0.4904 - accuracy: 0.7608 - auc: 0.8445 - val_loss: 0.4899 - val_accuracy: 0.7611 - val_auc: 0.8450\n",
      "Epoch 10/1000\n",
      "7699997/7699997 [==============================] - 57s 7us/sample - loss: 0.4886 - accuracy: 0.7622 - auc: 0.8459 - val_loss: 0.4887 - val_accuracy: 0.7618 - val_auc: 0.8464\n",
      "Epoch 11/1000\n",
      "7699997/7699997 [==============================] - 57s 7us/sample - loss: 0.4871 - accuracy: 0.7632 - auc: 0.8471 - val_loss: 0.4858 - val_accuracy: 0.7639 - val_auc: 0.8481\n",
      "Epoch 12/1000\n",
      "7699997/7699997 [==============================] - 59s 8us/sample - loss: 0.4857 - accuracy: 0.7642 - auc: 0.8482 - val_loss: 0.4859 - val_accuracy: 0.7637 - val_auc: 0.8486\n",
      "Epoch 13/1000\n",
      "7699997/7699997 [==============================] - 59s 8us/sample - loss: 0.4845 - accuracy: 0.7650 - auc: 0.8490 - val_loss: 0.4857 - val_accuracy: 0.7640 - val_auc: 0.8483\n",
      "Epoch 14/1000\n",
      "7699997/7699997 [==============================] - 58s 8us/sample - loss: 0.4834 - accuracy: 0.7657 - auc: 0.8499 - val_loss: 0.4844 - val_accuracy: 0.7652 - val_auc: 0.8495\n",
      "Epoch 15/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4824 - accuracy: 0.7664 - auc: 0.8506 - val_loss: 0.4852 - val_accuracy: 0.7644 - val_auc: 0.8498\n",
      "Epoch 16/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4816 - accuracy: 0.7670 - auc: 0.8513 - val_loss: 0.4836 - val_accuracy: 0.7658 - val_auc: 0.8501\n",
      "Epoch 17/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4808 - accuracy: 0.7675 - auc: 0.8519 - val_loss: 0.4824 - val_accuracy: 0.7665 - val_auc: 0.8508\n",
      "Epoch 18/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4801 - accuracy: 0.7680 - auc: 0.8525 - val_loss: 0.4808 - val_accuracy: 0.7676 - val_auc: 0.8520\n",
      "Epoch 19/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4794 - accuracy: 0.7686 - auc: 0.8530 - val_loss: 0.4811 - val_accuracy: 0.7675 - val_auc: 0.8519\n",
      "Epoch 20/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4788 - accuracy: 0.7689 - auc: 0.8534 - val_loss: 0.4807 - val_accuracy: 0.7677 - val_auc: 0.8523\n",
      "Epoch 21/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4782 - accuracy: 0.7693 - auc: 0.8539 - val_loss: 0.4797 - val_accuracy: 0.7683 - val_auc: 0.8530\n",
      "Epoch 22/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4776 - accuracy: 0.7699 - auc: 0.8543 - val_loss: 0.4785 - val_accuracy: 0.7693 - val_auc: 0.8537\n",
      "Epoch 23/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4771 - accuracy: 0.7701 - auc: 0.8547 - val_loss: 0.4790 - val_accuracy: 0.7691 - val_auc: 0.8536\n",
      "Epoch 24/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4765 - accuracy: 0.7705 - auc: 0.8551 - val_loss: 0.4786 - val_accuracy: 0.7690 - val_auc: 0.8541\n",
      "Epoch 25/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4760 - accuracy: 0.7708 - auc: 0.8555 - val_loss: 0.4784 - val_accuracy: 0.7694 - val_auc: 0.8541\n",
      "Epoch 26/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4756 - accuracy: 0.7712 - auc: 0.8558 - val_loss: 0.4773 - val_accuracy: 0.7702 - val_auc: 0.8548\n",
      "Epoch 27/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4751 - accuracy: 0.7714 - auc: 0.8562 - val_loss: 0.4779 - val_accuracy: 0.7696 - val_auc: 0.8547\n",
      "Epoch 28/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4746 - accuracy: 0.7718 - auc: 0.8565 - val_loss: 0.4765 - val_accuracy: 0.7706 - val_auc: 0.8554\n",
      "Epoch 29/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4742 - accuracy: 0.7720 - auc: 0.8568 - val_loss: 0.4763 - val_accuracy: 0.7706 - val_auc: 0.8554\n",
      "Epoch 30/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4738 - accuracy: 0.7723 - auc: 0.8571 - val_loss: 0.4760 - val_accuracy: 0.7710 - val_auc: 0.8557\n",
      "Epoch 31/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4734 - accuracy: 0.7727 - auc: 0.8574 - val_loss: 0.4757 - val_accuracy: 0.7712 - val_auc: 0.8559\n",
      "Epoch 32/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4730 - accuracy: 0.7729 - auc: 0.8577 - val_loss: 0.4754 - val_accuracy: 0.7713 - val_auc: 0.8562\n",
      "Epoch 33/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4725 - accuracy: 0.7732 - auc: 0.8580 - val_loss: 0.4755 - val_accuracy: 0.7712 - val_auc: 0.8561\n",
      "Epoch 34/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4721 - accuracy: 0.7734 - auc: 0.8583 - val_loss: 0.4746 - val_accuracy: 0.7719 - val_auc: 0.8569\n",
      "Epoch 35/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4717 - accuracy: 0.7738 - auc: 0.8585 - val_loss: 0.4754 - val_accuracy: 0.7714 - val_auc: 0.8568\n",
      "Epoch 36/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4713 - accuracy: 0.7740 - auc: 0.8589 - val_loss: 0.4735 - val_accuracy: 0.7726 - val_auc: 0.8574\n",
      "Epoch 37/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4709 - accuracy: 0.7742 - auc: 0.8591 - val_loss: 0.4736 - val_accuracy: 0.7724 - val_auc: 0.8573\n",
      "Epoch 38/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4705 - accuracy: 0.7745 - auc: 0.8594 - val_loss: 0.4738 - val_accuracy: 0.7724 - val_auc: 0.8577\n",
      "Epoch 39/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4701 - accuracy: 0.7748 - auc: 0.8597 - val_loss: 0.4736 - val_accuracy: 0.7725 - val_auc: 0.8574\n",
      "Epoch 40/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4697 - accuracy: 0.7750 - auc: 0.8599 - val_loss: 0.4725 - val_accuracy: 0.7732 - val_auc: 0.8581\n",
      "Epoch 41/1000\n",
      "7699997/7699997 [==============================] - 61s 8us/sample - loss: 0.4693 - accuracy: 0.7753 - auc: 0.8602 - val_loss: 0.4728 - val_accuracy: 0.7730 - val_auc: 0.8580\n",
      "Epoch 42/1000\n",
      "7699997/7699997 [==============================] - 61s 8us/sample - loss: 0.4689 - accuracy: 0.7756 - auc: 0.8605 - val_loss: 0.4724 - val_accuracy: 0.7734 - val_auc: 0.8583\n",
      "Epoch 43/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4685 - accuracy: 0.7758 - auc: 0.8608 - val_loss: 0.4726 - val_accuracy: 0.7731 - val_auc: 0.8585\n",
      "Epoch 44/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4681 - accuracy: 0.7761 - auc: 0.8610 - val_loss: 0.4719 - val_accuracy: 0.7736 - val_auc: 0.8587\n",
      "Epoch 45/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4677 - accuracy: 0.7763 - auc: 0.8613 - val_loss: 0.4713 - val_accuracy: 0.7740 - val_auc: 0.8588\n",
      "Epoch 46/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4672 - accuracy: 0.7766 - auc: 0.8616 - val_loss: 0.4711 - val_accuracy: 0.7741 - val_auc: 0.8591\n",
      "Epoch 47/1000\n",
      "7699997/7699997 [==============================] - 61s 8us/sample - loss: 0.4669 - accuracy: 0.7769 - auc: 0.8618 - val_loss: 0.4707 - val_accuracy: 0.7745 - val_auc: 0.8593\n",
      "Epoch 48/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4665 - accuracy: 0.7770 - auc: 0.8621 - val_loss: 0.4705 - val_accuracy: 0.7745 - val_auc: 0.8594\n",
      "Epoch 49/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4661 - accuracy: 0.7773 - auc: 0.8624 - val_loss: 0.4703 - val_accuracy: 0.7746 - val_auc: 0.8595\n",
      "Epoch 50/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4660 - accuracy: 0.7774 - auc: 0.8625 - val_loss: 0.4703 - val_accuracy: 0.7746 - val_auc: 0.8595\n",
      "Epoch 51/1000\n",
      "7699997/7699997 [==============================] - 60s 8us/sample - loss: 0.4660 - accuracy: 0.7774 - auc: 0.8625 - val_loss: 0.4703 - val_accuracy: 0.7746 - val_auc: 0.8595\n",
      "Epoch 52/1000\n",
      "7699997/7699997 [==============================] - 61s 8us/sample - loss: 0.4660 - accuracy: 0.7774 - auc: 0.8625 - val_loss: 0.4703 - val_accuracy: 0.7746 - val_auc: 0.8595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b1479d4608>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN\n",
    "model.fit(x_train_split, y_train_split, batch_size = batch_size, validation_data=(x_test_split,y_test_split), epochs = 1000,callbacks = callbacks_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the paper they had a complete AUC of 0.876. We were able to acchieve a comparable AUC of 0.8595"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancements to the Network\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. As discussed in the class, SGD does the job but takes a lot of time to converge. Also the use of a fixed momentum limited our performance in this case. There are many other optimization algorithms that are known to perform faster and better, such as Adam, Nadam, Adagard, Adamax. All these optimazition algorithms are in some way based on SDG, but with variations to make the convergence faster. 'AD' in the most of the above mentioned algorithms stand for 'Adaptive' Momentum. For example: Adam keeps the average of 'n' exponentially decaying average of the past gradients and adjust the learning rate accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trying to run the network with relu as the activation function and Adadelta just for 10 Epochs to see if it converges any faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 300)               8700      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 189,601\n",
      "Trainable params: 189,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(tf.keras.Input(shape=(28,)))\n",
    "\n",
    "model1.add(Dense(300,activation = 'relu',kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay),kernel_initializer = initializer_hidden_layers))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model1.add(Dense(300,activation = 'relu',kernel_initializer = initializer_hidden_layers ,kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model1.add(Dense(300,activation = 'relu', kernel_initializer = initializer_hidden_layers, kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model1.add(Dense(1,activation = 'sigmoid',kernel_initializer = initializer_output))\n",
    "\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "model1.compile(optimizer = optimizers.Adadelta(), loss = 'binary_crossentropy', metrics = ['accuracy',tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7699997 samples, validate on 3300000 samples\n",
      "Epoch 1/10\n",
      "7699997/7699997 [==============================] - 59s 8us/sample - loss: 0.6966 - accuracy: 0.5301 - auc_2: 0.5271 - val_loss: 0.6953 - val_accuracy: 0.5305 - val_auc_2: 0.5827\n",
      "Epoch 2/10\n",
      "7699997/7699997 [==============================] - 56s 7us/sample - loss: 0.6932 - accuracy: 0.5431 - auc_2: 0.5996 - val_loss: 0.6903 - val_accuracy: 0.5627 - val_auc_2: 0.6179\n",
      "Epoch 3/10\n",
      "7699997/7699997 [==============================] - 56s 7us/sample - loss: 0.6843 - accuracy: 0.5810 - auc_2: 0.6267 - val_loss: 0.6763 - val_accuracy: 0.5964 - val_auc_2: 0.6422\n",
      "Epoch 4/10\n",
      "7699997/7699997 [==============================] - 57s 7us/sample - loss: 0.6648 - accuracy: 0.6099 - auc_2: 0.6572 - val_loss: 0.6529 - val_accuracy: 0.6234 - val_auc_2: 0.6734\n",
      "Epoch 5/10\n",
      "7699997/7699997 [==============================] - 56s 7us/sample - loss: 0.6439 - accuracy: 0.6344 - auc_2: 0.6850 - val_loss: 0.6368 - val_accuracy: 0.6424 - val_auc_2: 0.6945\n",
      "Epoch 6/10\n",
      "7699997/7699997 [==============================] - 56s 7us/sample - loss: 0.6325 - accuracy: 0.6473 - auc_2: 0.7006 - val_loss: 0.6288 - val_accuracy: 0.6514 - val_auc_2: 0.7061\n",
      "Epoch 7/10\n",
      "7699997/7699997 [==============================] - 56s 7us/sample - loss: 0.6259 - accuracy: 0.6551 - auc_2: 0.7105 - val_loss: 0.6232 - val_accuracy: 0.6582 - val_auc_2: 0.7145\n",
      "Epoch 8/10\n",
      "7699997/7699997 [==============================] - 56s 7us/sample - loss: 0.6208 - accuracy: 0.6612 - auc_2: 0.7180 - val_loss: 0.6186 - val_accuracy: 0.6636 - val_auc_2: 0.7212\n",
      "Epoch 9/10\n",
      "7699997/7699997 [==============================] - 56s 7us/sample - loss: 0.6164 - accuracy: 0.6661 - auc_2: 0.7241 - val_loss: 0.6144 - val_accuracy: 0.6681 - val_auc_2: 0.7268\n",
      "Epoch 10/10\n",
      "7699997/7699997 [==============================] - 56s 7us/sample - loss: 0.6124 - accuracy: 0.6701 - auc_2: 0.7294 - val_loss: 0.6105 - val_accuracy: 0.6718 - val_auc_2: 0.7317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b318d0f408>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN\n",
    "model1.fit(x_train_split, y_train_split, batch_size = batch_size, validation_data=(x_test_split,y_test_split), epochs = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Model Training on Epoch 1 starts with low Accuracy and AUC, but the adaptable momentum benifits are definately seen as the training progresses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Number of nodes in the network: It has been mentioned that various combinations of number of nodes and layers were tried, however a deeper neural network could be worth trying to see if accuracy increases with increasing number of nodes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will try to train the network that has 500 Nodes in each layer and has one extra layer to see if it makes any difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 500)               14500     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 766,501\n",
      "Trainable params: 766,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "callbacks_model2 = [earlyStopping]\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(tf.keras.Input(shape=(28,)))\n",
    "\n",
    "model2.add(Dense(500,activation = 'relu',kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay),kernel_initializer = initializer_hidden_layers))\n",
    "\n",
    "model2.add(Dense(500,activation = 'relu',kernel_initializer = initializer_hidden_layers ,kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "\n",
    "model2.add(Dense(500,activation = 'relu', kernel_initializer = initializer_hidden_layers, kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "\n",
    "model2.add(Dense(500,activation = 'relu', kernel_initializer = initializer_hidden_layers, kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "\n",
    "model2.add(Dense(1,activation = 'sigmoid',kernel_initializer = initializer_output))\n",
    "\n",
    "# Compiling the ANN\n",
    "model2.compile(optimizer = optimizers.Adadelta(), loss = 'binary_crossentropy', metrics = ['accuracy',tf.keras.metrics.AUC()])\n",
    "\n",
    "# # Fitting the ANN\n",
    "# model2.fit(x_train_split, y_train_split, batch_size = batch_size, validation_data=(x_test_split,y_test_split), epochs = 1000, callbacks = callbacks_model2)\n",
    "\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Capture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We ran the above code. The training loss and increase in AUC looked promising in the initial iterations, however for a time Val accuracy was around 0.8335. Therefore we had to stop the training. Threre was a early stopping method set as a callback, however as the loss was slightly decreasing, the training kept on running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cyclic Learning Rate: The learning rate usually goes from high to low in the decay process, hoping that the local minimum was already traversed when the Learning Rate was high enough, but it might be the case that model is being stuck in a local minimum. Cyclin learning rate oscillates in the cycles of highs and lows. Therefore even if the model gets stuck in the local minimum, the upcoming cycle of hyigh Learning rate can get the model out of the local minimum and the model can go towards global minimum.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will try to implement Traingular Learning Rate over Epochs to see if the convergence can be any faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateBatchScheduler_traingular(tf.keras.callbacks.Callback):\n",
    "  \n",
    "    def __init__(self, update_freq=None):\n",
    "        self._update_freq = update_freq\n",
    "        self.lr = lr\n",
    "        self.iteration = 1\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self._update_freq and batch % self._update_freq != 0:\n",
    "            return\n",
    "        # To avoid divergence, limit the value range.\n",
    "        \n",
    "        if self.lr <= min_lr:\n",
    "            self.lr = lr\n",
    "        else:    \n",
    "            self.lr = self.lr - lr_decay_factor\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)\n",
    "#         print(\"lr:\",self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7699997 samples, validate on 3300000 samples\n",
      "Epoch 1/1000\n",
      "7699997/7699997 [==============================] - 32s 4us/sample - loss: 0.5075 - accuracy: 0.7484 - auc_7: 0.8308 - val_loss: 0.4881 - val_accuracy: 0.7623 - val_auc_7: 0.8466\n",
      "Epoch 2/1000\n",
      "7699997/7699997 [==============================] - 31s 4us/sample - loss: 0.4846 - accuracy: 0.7649 - auc_7: 0.8491 - val_loss: 0.4831 - val_accuracy: 0.7667 - val_auc_7: 0.8510\n",
      "Epoch 3/1000\n",
      "7699997/7699997 [==============================] - 31s 4us/sample - loss: 0.4790 - accuracy: 0.7692 - auc_7: 0.8537 - val_loss: 0.4792 - val_accuracy: 0.7690 - val_auc_7: 0.8544\n",
      "Epoch 4/1000\n",
      "7699997/7699997 [==============================] - 31s 4us/sample - loss: 0.4758 - accuracy: 0.7717 - auc_7: 0.8564 - val_loss: 0.4755 - val_accuracy: 0.7719 - val_auc_7: 0.8569\n",
      "Epoch 5/1000\n",
      "7699997/7699997 [==============================] - 31s 4us/sample - loss: 0.4737 - accuracy: 0.7734 - auc_7: 0.8581 - val_loss: 0.4737 - val_accuracy: 0.7736 - val_auc_7: 0.8583\n",
      "Epoch 6/1000\n",
      "7699997/7699997 [==============================] - 32s 4us/sample - loss: 0.4723 - accuracy: 0.7745 - auc_7: 0.8593 - val_loss: 0.4722 - val_accuracy: 0.7746 - val_auc_7: 0.8595\n",
      "Epoch 7/1000\n",
      "7699997/7699997 [==============================] - 29s 4us/sample - loss: 0.4711 - accuracy: 0.7754 - auc_7: 0.8603 - val_loss: 0.4722 - val_accuracy: 0.7747 - val_auc_7: 0.8598\n",
      "Epoch 8/1000\n",
      "7699997/7699997 [==============================] - 29s 4us/sample - loss: 0.4700 - accuracy: 0.7762 - auc_7: 0.8612 - val_loss: 0.4709 - val_accuracy: 0.7759 - val_auc_7: 0.8607\n",
      "Epoch 9/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4691 - accuracy: 0.7769 - auc_7: 0.8619 - val_loss: 0.4708 - val_accuracy: 0.7761 - val_auc_7: 0.8613\n",
      "Epoch 10/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4683 - accuracy: 0.7776 - auc_7: 0.8626 - val_loss: 0.4714 - val_accuracy: 0.7756 - val_auc_7: 0.8616\n",
      "Epoch 11/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4674 - accuracy: 0.7781 - auc_7: 0.8632 - val_loss: 0.4699 - val_accuracy: 0.7769 - val_auc_7: 0.8626\n",
      "Epoch 12/1000\n",
      "7699997/7699997 [==============================] - 31s 4us/sample - loss: 0.4669 - accuracy: 0.7787 - auc_7: 0.8637 - val_loss: 0.4692 - val_accuracy: 0.7774 - val_auc_7: 0.8627\n",
      "Epoch 13/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4664 - accuracy: 0.7790 - auc_7: 0.8640 - val_loss: 0.4680 - val_accuracy: 0.7777 - val_auc_7: 0.8629\n",
      "Epoch 14/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4661 - accuracy: 0.7794 - auc_7: 0.8643 - val_loss: 0.4674 - val_accuracy: 0.7784 - val_auc_7: 0.8635\n",
      "Epoch 15/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4658 - accuracy: 0.7795 - auc_7: 0.8646 - val_loss: 0.4657 - val_accuracy: 0.7797 - val_auc_7: 0.8647\n",
      "Epoch 16/1000\n",
      "7699997/7699997 [==============================] - 31s 4us/sample - loss: 0.4655 - accuracy: 0.7798 - auc_7: 0.8648 - val_loss: 0.4671 - val_accuracy: 0.7786 - val_auc_7: 0.8642\n",
      "Epoch 17/1000\n",
      "7699997/7699997 [==============================] - 31s 4us/sample - loss: 0.4652 - accuracy: 0.7801 - auc_7: 0.8650 - val_loss: 0.4667 - val_accuracy: 0.7787 - val_auc_7: 0.8640\n",
      "Epoch 18/1000\n",
      "7699997/7699997 [==============================] - 31s 4us/sample - loss: 0.4650 - accuracy: 0.7800 - auc_7: 0.8651 - val_loss: 0.4662 - val_accuracy: 0.7792 - val_auc_7: 0.8644\n",
      "Epoch 19/1000\n",
      "7699997/7699997 [==============================] - 31s 4us/sample - loss: 0.4648 - accuracy: 0.7802 - auc_7: 0.8653 - val_loss: 0.4662 - val_accuracy: 0.7794 - val_auc_7: 0.8646\n",
      "Epoch 20/1000\n",
      "7699997/7699997 [==============================] - 31s 4us/sample - loss: 0.4647 - accuracy: 0.7804 - auc_7: 0.8654 - val_loss: 0.4665 - val_accuracy: 0.7791 - val_auc_7: 0.8646\n",
      "Epoch 21/1000\n",
      "7699997/7699997 [==============================] - 31s 4us/sample - loss: 0.4646 - accuracy: 0.7804 - auc_7: 0.8655 - val_loss: 0.4656 - val_accuracy: 0.7795 - val_auc_7: 0.8650\n",
      "Epoch 22/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4644 - accuracy: 0.7805 - auc_7: 0.8656 - val_loss: 0.4664 - val_accuracy: 0.7792 - val_auc_7: 0.8646\n",
      "Epoch 23/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4643 - accuracy: 0.7806 - auc_7: 0.8657 - val_loss: 0.4665 - val_accuracy: 0.7791 - val_auc_7: 0.8646\n",
      "Epoch 24/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4642 - accuracy: 0.7806 - auc_7: 0.8658 - val_loss: 0.4659 - val_accuracy: 0.7796 - val_auc_7: 0.8648\n",
      "Epoch 25/1000\n",
      "7699997/7699997 [==============================] - 31s 4us/sample - loss: 0.4641 - accuracy: 0.7807 - auc_7: 0.8658 - val_loss: 0.4656 - val_accuracy: 0.7797 - val_auc_7: 0.8649\n",
      "Epoch 26/1000\n",
      "7699997/7699997 [==============================] - 32s 4us/sample - loss: 0.4641 - accuracy: 0.7809 - auc_7: 0.8659 - val_loss: 0.4657 - val_accuracy: 0.7794 - val_auc_7: 0.8650\n",
      "Epoch 27/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4639 - accuracy: 0.7808 - auc_7: 0.8660 - val_loss: 0.4651 - val_accuracy: 0.7798 - val_auc_7: 0.8652\n",
      "Epoch 28/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4638 - accuracy: 0.7808 - auc_7: 0.8661 - val_loss: 0.4658 - val_accuracy: 0.7799 - val_auc_7: 0.8648\n",
      "Epoch 29/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4638 - accuracy: 0.7809 - auc_7: 0.8660 - val_loss: 0.4651 - val_accuracy: 0.7801 - val_auc_7: 0.8652\n",
      "Epoch 30/1000\n",
      "7699997/7699997 [==============================] - 30s 4us/sample - loss: 0.4637 - accuracy: 0.7810 - auc_7: 0.8661 - val_loss: 0.4649 - val_accuracy: 0.7801 - val_auc_7: 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b327d028c8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks_model3 = [ LearningRateBatchScheduler_traingular(), earlyStopping]\n",
    "\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(tf.keras.Input(shape=(28,)))\n",
    "\n",
    "model3.add(Dense(300,activation = 'relu',kernel_initializer = initializer_hidden_layers ,kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Dense(300,activation = 'relu', kernel_initializer = initializer_hidden_layers, kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Dense(300,activation = 'relu', kernel_initializer = initializer_hidden_layers, kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Dense(1,activation = 'sigmoid',kernel_initializer = initializer_output))\n",
    "\n",
    "# Compiling the ANN\n",
    "model3.compile(optimizer = optimizers.Adam(), loss = 'binary_crossentropy', metrics = ['accuracy',tf.keras.metrics.AUC()])\n",
    "\n",
    "# Fitting the ANN\n",
    "model3.fit(x_train_split, y_train_split, batch_size = 1000, validation_data=(x_test_split,y_test_split), epochs = 1000, callbacks = callbacks_model2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Combination of Adam and the Traingular learning rate got the Validation AUC in the range of 0.86 relative faster, but did not go over 0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Despite of some changes in the libraries, we were able to get very close to the AUC in the Paper \n",
    "\n",
    "* Adding the Dropout layer actually made our model give a slightly less AUC\n",
    "\n",
    "* The Enhancements to the model that differ from the paper - such as Adding more nodes and layers, using different optimization algorithms such as Adam and Admax and using triangular learning rate, gave us very close results however the combination of Adam with the traingular learning rate reached the AUC of 0.86 plus in just 8 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ADEC]",
   "language": "python",
   "name": "conda-env-ADEC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
