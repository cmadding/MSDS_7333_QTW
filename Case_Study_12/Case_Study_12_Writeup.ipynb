{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 12: Case Study "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following paper: https://arxiv.org/pdf/1402.4735.pdf \n",
    "\n",
    "Build a replica Neural Network with the paper’s architecture using Tensorflow. \n",
    "\n",
    "If possible, begin to train on the data located here: https://archive.ics.uci.edu/ml/datasets/HIGGS \n",
    "\n",
    "How close can you get to the original results? \n",
    "\n",
    "To facilitate quicker training, you may increase the batch size temporarily (this has a small impact on the final result but can speed your calculations significantly). You do not need to train a final result using the paper’s parameters; only the code for your model is required in your final submission. \n",
    "\n",
    "Include in your report: \n",
    "Based on the class notes and discussion, suggest improvements to the procedure. \n",
    "What are standard practices now versus when this paper was written? \n",
    "What kind of improvements do they provide? \n",
    "How would you quantify if your result duplicated the papers? \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract \n",
    "\n",
    "We will be carrying out a classification problem to distinguish between a signal process that produces Higgs bosons and a background process that does not. Our information is based on outdated code, so in the process, we will reproduce results with more updated information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are standard practices now versus when this paper was written? \n",
    "\n",
    "In the paper “Searching for Exotic Particles in High-Energy Physics with Deep Learning,” published in 2014, the authors looked at using deep learning models to assist in classifying “exotic particle(s)” that result from particle collisions. Their paper focused on using the Higgs Boson data collected from the Large Hadron Collider’s detectors, and their study focused on improving past research that used ‘shallow’ machine learning models. According to the paper, these models have a limited capacity to learn complex non-linear functions. \n",
    "\n",
    "Their findings ended with Deep Learning outperforming the standard at the time, “shallow” models. The original research and building of the model used libraries standard in 2014. The original code, implemented through pylearn2, is no longer supported. \n",
    "\n",
    "In this project, we will use the same dataset and the Keras Python library with a TensorFlow backend to replicate their original work using more modern approaches. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras \n",
    "\n",
    "Keras is a neural network library written in Python and can run on top of TensorFlow and other existing toolkits. It is touted for its’ ease of use and can be up and running with just a few lines of code while still being customizable enough for layer by layer programming. \n",
    "\n",
    "### TensorFlow \n",
    "\n",
    "TensorFlow is a free and open-source platform developed by the Google Brain Team. It is a software library for dataflow and differentiable programming across a range of tasks and used for machine learning applications such as neural networks. (TensorFlow, n.d.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "\n",
    "The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the Neural Network in the Paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Five-layered neural network with 300 hidden units in each layer\n",
    "\n",
    "* The Activation function that was used in all layers and all nodes is 'tanh'\n",
    "\n",
    "* Weights were initialized from a normal distribution with zero mean and standard deviation 0.1 in the first layer, 0.001 in the output layer, and 0.05 all other hidden layers\n",
    "\n",
    "* Weight decay coefficient of 1 × 10^-5 was set for all the layers\n",
    "\n",
    "* Stocastic Gradient Descent was used as the optimizer\n",
    "\n",
    "* The initial learning rate was set to 0.05 and thereafter the learning rate was decayed by a factor of 1.0000002 every batch until it reached to the minimum learning rate of 10^-6\n",
    "\n",
    "* A momentum term increased linearly over the first 200 epochs from 0.9 to 0.99, at which point it remained constant\n",
    "\n",
    "* Gradient computations were made on mini-batches of size 100.\n",
    "\n",
    "* Training ended when the momentum had reached its maximum value and the minimum error on the validation set (500,000 examples) had not decreased by more than a factor of 0.00001 over 10 epochs.\n",
    "\n",
    "* The Validation size was 500,000\n",
    "\n",
    "* The Neural Network was created using the Theano library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Creation of the Neural Network in Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations and Workarounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. There was no option that was found to dynamically adjust the momentum in keras, therefore the standard SGD momentum was used \n",
    " \n",
    " 2. Also, as a result of no momentum change, the early stopping condition that involves momentum could not be used\n",
    " \n",
    " 3. To replicate the lr decay, the 'LR Scdeduler' that is provided by Keras callback functions was earlier used but that decays the learning rate per Epoch which did not fit our requirement\n",
    " \n",
    " 4. To adress the above problem, a class `LearningRateBatchScheduler` was inherited from `(tf.keras.callbacks.Callback)`. The on_batch_end method was used to make sure that the change in Learning Rate happens at the end of the batch. The print statement in the code (which is now commented) was used to verify the same\n",
    " \n",
    " 5. The Weight Decay were set by using l2 function in the kernel regularizer \n",
    " \n",
    " 6. For setting the initial weights in the different laters, per the normal discribution and different standard deviations mentioned in the paper, `kernel_initializer` parameter in the dense layer was used   \n",
    " \n",
    " 7. We used the Validation set that is 30% of the total dataset, just to speed up the training by reducing the number of records in the test set\n",
    " \n",
    " 8. Batch size in the paper was 100, but again for speeding up the training we will be using bath size of 1000\n",
    " \n",
    " 9. The minimum change in loss for early stopping, was increased to '0.0001' , from '0.00001' and the patience was decreased to 2 for stopping the training earlier as the Network was still getting decent AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the data \n",
    "\n",
    "df_data = pd.read_hdf('data.h5', 'table', where=['index>2'])\n",
    "\n",
    "np_data = np.array(df_data.iloc[:,1:])\n",
    "lables = np.array(df_data.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>lepton pt</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10999995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.160156</td>\n",
       "      <td>1.013672</td>\n",
       "      <td>0.108643</td>\n",
       "      <td>1.495117</td>\n",
       "      <td>-0.537598</td>\n",
       "      <td>2.341797</td>\n",
       "      <td>-0.839844</td>\n",
       "      <td>1.320312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097046</td>\n",
       "      <td>1.190430</td>\n",
       "      <td>3.101562</td>\n",
       "      <td>0.822266</td>\n",
       "      <td>0.766602</td>\n",
       "      <td>1.001953</td>\n",
       "      <td>1.061523</td>\n",
       "      <td>0.836914</td>\n",
       "      <td>0.860352</td>\n",
       "      <td>0.772461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618164</td>\n",
       "      <td>-1.012695</td>\n",
       "      <td>1.110352</td>\n",
       "      <td>0.940918</td>\n",
       "      <td>-0.379150</td>\n",
       "      <td>1.004883</td>\n",
       "      <td>0.348633</td>\n",
       "      <td>-1.678711</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217041</td>\n",
       "      <td>1.048828</td>\n",
       "      <td>3.101562</td>\n",
       "      <td>0.826660</td>\n",
       "      <td>0.989746</td>\n",
       "      <td>1.029297</td>\n",
       "      <td>1.199219</td>\n",
       "      <td>0.891602</td>\n",
       "      <td>0.938477</td>\n",
       "      <td>0.865234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700684</td>\n",
       "      <td>0.774414</td>\n",
       "      <td>1.520508</td>\n",
       "      <td>0.847168</td>\n",
       "      <td>0.211182</td>\n",
       "      <td>1.095703</td>\n",
       "      <td>0.052460</td>\n",
       "      <td>0.024551</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>1.584961</td>\n",
       "      <td>1.713867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337402</td>\n",
       "      <td>0.845215</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.883301</td>\n",
       "      <td>1.888672</td>\n",
       "      <td>1.153320</td>\n",
       "      <td>0.931152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.177734</td>\n",
       "      <td>0.117798</td>\n",
       "      <td>-1.277344</td>\n",
       "      <td>1.864258</td>\n",
       "      <td>-0.584473</td>\n",
       "      <td>0.998535</td>\n",
       "      <td>-1.264648</td>\n",
       "      <td>1.276367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.399414</td>\n",
       "      <td>-1.313477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838867</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>1.201172</td>\n",
       "      <td>0.939453</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>0.759277</td>\n",
       "      <td>0.719238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464355</td>\n",
       "      <td>-0.337158</td>\n",
       "      <td>0.229004</td>\n",
       "      <td>0.954590</td>\n",
       "      <td>-0.868652</td>\n",
       "      <td>0.429932</td>\n",
       "      <td>-0.271240</td>\n",
       "      <td>-1.251953</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.652344</td>\n",
       "      <td>-0.586426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.752441</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.986816</td>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.576172</td>\n",
       "      <td>0.541504</td>\n",
       "      <td>0.517578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label  lepton pt  lepton eta  lepton phi  missing energy magnitude  \\\n",
       "10999995    1.0   1.160156    1.013672    0.108643                  1.495117   \n",
       "10999996    1.0   0.618164   -1.012695    1.110352                  0.940918   \n",
       "10999997    1.0   0.700684    0.774414    1.520508                  0.847168   \n",
       "10999998    0.0   1.177734    0.117798   -1.277344                  1.864258   \n",
       "10999999    0.0   0.464355   -0.337158    0.229004                  0.954590   \n",
       "\n",
       "          missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag  \\\n",
       "10999995           -0.537598  2.341797  -0.839844   1.320312     0.000000   \n",
       "10999996           -0.379150  1.004883   0.348633  -1.678711     2.173828   \n",
       "10999997            0.211182  1.095703   0.052460   0.024551     2.173828   \n",
       "10999998           -0.584473  0.998535  -1.264648   1.276367     0.000000   \n",
       "10999999           -0.868652  0.429932  -0.271240  -1.251953     2.173828   \n",
       "\n",
       "          ...  jet 4 eta  jet 4 phi  jet 4 b-tag      m_jj     m_jjj  \\\n",
       "10999995  ...  -0.097046   1.190430     3.101562  0.822266  0.766602   \n",
       "10999996  ...  -0.217041   1.048828     3.101562  0.826660  0.989746   \n",
       "10999997  ...   1.584961   1.713867     0.000000  0.337402  0.845215   \n",
       "10999998  ...   1.399414  -1.313477     0.000000  0.838867  0.882812   \n",
       "10999999  ...  -1.652344  -0.586426     0.000000  0.752441  0.740723   \n",
       "\n",
       "              m_lv     m_jlv      m_bb     m_wbb    m_wwbb  \n",
       "10999995  1.001953  1.061523  0.836914  0.860352  0.772461  \n",
       "10999996  1.029297  1.199219  0.891602  0.938477  0.865234  \n",
       "10999997  0.987793  0.883301  1.888672  1.153320  0.931152  \n",
       "10999998  1.201172  0.939453  0.339600  0.759277  0.719238  \n",
       "10999999  0.986816  0.664062  0.576172  0.541504  0.517578  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "np_data_scaled = sc.fit_transform(np_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(np_data_scaled, lables, test_size = 0.30, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.utils import normalize, to_categorical\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nueral Network Parameters\n",
    "\n",
    "batch_size = 500 #Batch Size modified from what was mentioned in the paper to speed up the training process\n",
    "weights_decay = 0.00001\n",
    "\n",
    "lr = 0.15\n",
    "min_lr = 0.000001\n",
    "lr_decay_factor = 0.0000002\n",
    "\n",
    "\n",
    "initializer_input = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.1)\n",
    "initializer_output = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.001)\n",
    "initializer_hidden_layers = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a class as a clid class of 'Callbacks' to change the Learning Rate on every batch\n",
    "\n",
    "class LearningRateBatchScheduler(tf.keras.callbacks.Callback):\n",
    "  \n",
    "    def __init__(self, update_freq=None):\n",
    "        self._update_freq = update_freq\n",
    "        self.lr =lr\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self._update_freq and batch % self._update_freq != 0:\n",
    "            return\n",
    "        # To avoid divergence, limit the value range.\n",
    "        \n",
    "        if self.lr <= min_lr:\n",
    "            self.lr = min_lr\n",
    "        else:    \n",
    "            self.lr = self.lr - lr_decay_factor\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)\n",
    "#         print(\"lr:\",self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Early Stopping \n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='loss', mode='min', min_delta = 0.0001 , patience = 2)\n",
    "\n",
    "callbacks_a = [ LearningRateBatchScheduler(), earlyStopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 300)               8700      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 189,601\n",
      "Trainable params: 189,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(tf.keras.Input(shape=(28,)))\n",
    "\n",
    "model.add(Dense(300,activation = 'tanh',kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay),kernel_initializer = initializer_hidden_layers))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(300,activation = 'tanh',kernel_initializer = initializer_hidden_layers ,kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(300,activation = 'tanh', kernel_initializer = initializer_hidden_layers, kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1,activation = 'sigmoid',kernel_initializer = initializer_output))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "model.compile(optimizer = optimizers.SGD(), loss = 'binary_crossentropy', metrics = ['accuracy',tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7699997 samples, validate on 3300000 samples\n",
      "Epoch 1/1000\n",
      "7699997/7699997 [==============================] - 66s 9us/sample - loss: 0.5567 - accuracy: 0.7128 - auc_1: 0.7878 - val_loss: 0.5240 - val_accuracy: 0.7376 - val_auc_1: 0.8179\n",
      "Epoch 2/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.5164 - accuracy: 0.7429 - auc_1: 0.8240 - val_loss: 0.5149 - val_accuracy: 0.7434 - val_auc_1: 0.8264\n",
      "Epoch 3/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.5062 - accuracy: 0.7498 - auc_1: 0.8321 - val_loss: 0.5026 - val_accuracy: 0.7523 - val_auc_1: 0.8352\n",
      "Epoch 4/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.5008 - accuracy: 0.7536 - auc_1: 0.8364 - val_loss: 0.4984 - val_accuracy: 0.7552 - val_auc_1: 0.8386\n",
      "Epoch 5/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4971 - accuracy: 0.7563 - auc_1: 0.8394 - val_loss: 0.4950 - val_accuracy: 0.7578 - val_auc_1: 0.8411\n",
      "Epoch 6/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4941 - accuracy: 0.7583 - auc_1: 0.8417 - val_loss: 0.4917 - val_accuracy: 0.7597 - val_auc_1: 0.8436\n",
      "Epoch 7/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4915 - accuracy: 0.7602 - auc_1: 0.8437 - val_loss: 0.4910 - val_accuracy: 0.7606 - val_auc_1: 0.8446\n",
      "Epoch 8/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4894 - accuracy: 0.7615 - auc_1: 0.8454 - val_loss: 0.4893 - val_accuracy: 0.7617 - val_auc_1: 0.8459\n",
      "Epoch 9/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4876 - accuracy: 0.7629 - auc_1: 0.8468 - val_loss: 0.4909 - val_accuracy: 0.7608 - val_auc_1: 0.8468\n",
      "Epoch 10/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4860 - accuracy: 0.7640 - auc_1: 0.8480 - val_loss: 0.4849 - val_accuracy: 0.7650 - val_auc_1: 0.8489\n",
      "Epoch 11/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4846 - accuracy: 0.7651 - auc_1: 0.8491 - val_loss: 0.4840 - val_accuracy: 0.7654 - val_auc_1: 0.8496\n",
      "Epoch 12/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4834 - accuracy: 0.7660 - auc_1: 0.8500 - val_loss: 0.4850 - val_accuracy: 0.7651 - val_auc_1: 0.8498\n",
      "Epoch 13/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4823 - accuracy: 0.7667 - auc_1: 0.8509 - val_loss: 0.4844 - val_accuracy: 0.7651 - val_auc_1: 0.8504\n",
      "Epoch 14/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4812 - accuracy: 0.7672 - auc_1: 0.8516 - val_loss: 0.4838 - val_accuracy: 0.7654 - val_auc_1: 0.8500\n",
      "Epoch 15/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4803 - accuracy: 0.7680 - auc_1: 0.8524 - val_loss: 0.4815 - val_accuracy: 0.7669 - val_auc_1: 0.8515\n",
      "Epoch 16/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4795 - accuracy: 0.7685 - auc_1: 0.8529 - val_loss: 0.4802 - val_accuracy: 0.7682 - val_auc_1: 0.8525\n",
      "Epoch 17/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4788 - accuracy: 0.7690 - auc_1: 0.8535 - val_loss: 0.4806 - val_accuracy: 0.7678 - val_auc_1: 0.8528\n",
      "Epoch 18/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4781 - accuracy: 0.7695 - auc_1: 0.8540 - val_loss: 0.4790 - val_accuracy: 0.7689 - val_auc_1: 0.8535\n",
      "Epoch 19/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4775 - accuracy: 0.7700 - auc_1: 0.8545 - val_loss: 0.4806 - val_accuracy: 0.7683 - val_auc_1: 0.8533\n",
      "Epoch 20/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4768 - accuracy: 0.7705 - auc_1: 0.8550 - val_loss: 0.4789 - val_accuracy: 0.7690 - val_auc_1: 0.8540\n",
      "Epoch 21/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4763 - accuracy: 0.7708 - auc_1: 0.8554 - val_loss: 0.4775 - val_accuracy: 0.7702 - val_auc_1: 0.8549\n",
      "Epoch 22/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4757 - accuracy: 0.7713 - auc_1: 0.8559 - val_loss: 0.4774 - val_accuracy: 0.7701 - val_auc_1: 0.8547\n",
      "Epoch 23/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4752 - accuracy: 0.7717 - auc_1: 0.8562 - val_loss: 0.4768 - val_accuracy: 0.7708 - val_auc_1: 0.8554\n",
      "Epoch 24/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4747 - accuracy: 0.7720 - auc_1: 0.8566 - val_loss: 0.4764 - val_accuracy: 0.7707 - val_auc_1: 0.8556\n",
      "Epoch 25/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4742 - accuracy: 0.7723 - auc_1: 0.8570 - val_loss: 0.4765 - val_accuracy: 0.7705 - val_auc_1: 0.8559\n",
      "Epoch 26/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4737 - accuracy: 0.7726 - auc_1: 0.8573 - val_loss: 0.4763 - val_accuracy: 0.7708 - val_auc_1: 0.8555\n",
      "Epoch 27/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4733 - accuracy: 0.7729 - auc_1: 0.8576 - val_loss: 0.4764 - val_accuracy: 0.7708 - val_auc_1: 0.8557\n",
      "Epoch 28/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4729 - accuracy: 0.7732 - auc_1: 0.8580 - val_loss: 0.4743 - val_accuracy: 0.7724 - val_auc_1: 0.8571\n",
      "Epoch 29/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4724 - accuracy: 0.7736 - auc_1: 0.8583 - val_loss: 0.4747 - val_accuracy: 0.7720 - val_auc_1: 0.8567\n",
      "Epoch 30/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4720 - accuracy: 0.7739 - auc_1: 0.8586 - val_loss: 0.4746 - val_accuracy: 0.7723 - val_auc_1: 0.8568\n",
      "Epoch 31/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4716 - accuracy: 0.7741 - auc_1: 0.8588 - val_loss: 0.4741 - val_accuracy: 0.7724 - val_auc_1: 0.8574\n",
      "Epoch 32/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4712 - accuracy: 0.7744 - auc_1: 0.8591 - val_loss: 0.4735 - val_accuracy: 0.7729 - val_auc_1: 0.8576\n",
      "Epoch 33/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4708 - accuracy: 0.7745 - auc_1: 0.8594 - val_loss: 0.4748 - val_accuracy: 0.7723 - val_auc_1: 0.8577\n",
      "Epoch 34/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4704 - accuracy: 0.7749 - auc_1: 0.8597 - val_loss: 0.4733 - val_accuracy: 0.7730 - val_auc_1: 0.8579\n",
      "Epoch 35/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4700 - accuracy: 0.7751 - auc_1: 0.8599 - val_loss: 0.4736 - val_accuracy: 0.7729 - val_auc_1: 0.8579\n",
      "Epoch 36/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4696 - accuracy: 0.7754 - auc_1: 0.8602 - val_loss: 0.4732 - val_accuracy: 0.7730 - val_auc_1: 0.8578\n",
      "Epoch 37/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4692 - accuracy: 0.7757 - auc_1: 0.8605 - val_loss: 0.4729 - val_accuracy: 0.7734 - val_auc_1: 0.8584\n",
      "Epoch 38/1000\n",
      "7699997/7699997 [==============================] - 62s 8us/sample - loss: 0.4688 - accuracy: 0.7759 - auc_1: 0.8608 - val_loss: 0.4724 - val_accuracy: 0.7738 - val_auc_1: 0.8585\n",
      "Epoch 39/1000\n",
      "7699997/7699997 [==============================] - 61s 8us/sample - loss: 0.4685 - accuracy: 0.7761 - auc_1: 0.8610 - val_loss: 0.4725 - val_accuracy: 0.7735 - val_auc_1: 0.8583\n",
      "Epoch 40/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4680 - accuracy: 0.7763 - auc_1: 0.8613 - val_loss: 0.4721 - val_accuracy: 0.7739 - val_auc_1: 0.8587\n",
      "Epoch 41/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4676 - accuracy: 0.7766 - auc_1: 0.8616 - val_loss: 0.4712 - val_accuracy: 0.7743 - val_auc_1: 0.8591\n",
      "Epoch 42/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4672 - accuracy: 0.7769 - auc_1: 0.8618 - val_loss: 0.4712 - val_accuracy: 0.7744 - val_auc_1: 0.8592\n",
      "Epoch 43/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4668 - accuracy: 0.7772 - auc_1: 0.8621 - val_loss: 0.4711 - val_accuracy: 0.7745 - val_auc_1: 0.8593\n",
      "Epoch 44/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4664 - accuracy: 0.7773 - auc_1: 0.8624 - val_loss: 0.4708 - val_accuracy: 0.7746 - val_auc_1: 0.8596\n",
      "Epoch 45/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4660 - accuracy: 0.7776 - auc_1: 0.8626 - val_loss: 0.4702 - val_accuracy: 0.7749 - val_auc_1: 0.8598\n",
      "Epoch 46/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4656 - accuracy: 0.7778 - auc_1: 0.8629 - val_loss: 0.4698 - val_accuracy: 0.7752 - val_auc_1: 0.8601\n",
      "Epoch 47/1000\n",
      "7699997/7699997 [==============================] - 63s 8us/sample - loss: 0.4652 - accuracy: 0.7781 - auc_1: 0.8632 - val_loss: 0.4696 - val_accuracy: 0.7753 - val_auc_1: 0.8602\n",
      "Epoch 48/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4648 - accuracy: 0.7784 - auc_1: 0.8634 - val_loss: 0.4693 - val_accuracy: 0.7754 - val_auc_1: 0.8604\n",
      "Epoch 49/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4644 - accuracy: 0.7786 - auc_1: 0.8637 - val_loss: 0.4692 - val_accuracy: 0.7756 - val_auc_1: 0.8605\n",
      "Epoch 50/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4643 - accuracy: 0.7787 - auc_1: 0.8638 - val_loss: 0.4692 - val_accuracy: 0.7756 - val_auc_1: 0.8605\n",
      "Epoch 51/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4643 - accuracy: 0.7787 - auc_1: 0.8638 - val_loss: 0.4692 - val_accuracy: 0.7756 - val_auc_1: 0.8605\n",
      "Epoch 52/1000\n",
      "7699997/7699997 [==============================] - 64s 8us/sample - loss: 0.4643 - accuracy: 0.7787 - auc_1: 0.8638 - val_loss: 0.4692 - val_accuracy: 0.7756 - val_auc_1: 0.8605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c58bb68048>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN\n",
    "model.fit(x_train_split, y_train_split, batch_size = batch_size, validation_data=(x_test_split,y_test_split), epochs = 1000,callbacks = callbacks_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the paper they had a complete AUC of 0.876. We were able to acchieve a comparable AUC of 0.8605\n",
    "\n",
    "#### Enhancements to the Network:\n",
    "\n",
    "1. As discussed in the class, SGD does the job but takes a lot of time to converge. Also the use of a fixed momentum limited our performance in this case. There are many other optimization algorithms that are known to perform faster and better, such as Adam, Nadam, Adagard, Adamax. All these optimazition algorithms are in some way based on SDG, but with variations to make the convergence faster. 'AD' in the most of the above mentioned algorithms stand for 'Adaptive' Momentum. For example: Adam keeps the average of 'n' exponentially decaying average of the past gradients and adjust the learning rate accordingly.\n",
    "\n",
    "2. Number of nodes in the network: It has been mentioned that various combinations of number of nodes and layers were tried, however a deeper neural network could be worth trying to see if accuracy increases \n",
    "\n",
    "3. Cyclic Learning Rate: The learning rate usually goes from high to low in the decay process, hoping that the local minimum was already traversed when the Learning Rate was high enough, but it might be the case that model is being stuck in a local minimum. Cyclin learning rate oscillates in the cycles of highs and lows. Therefore even if the model gets stuck in the local minimum, the upcoming cycle of hyigh Learning rate can get the model out of the local minimum and the model can go towards global minimum.  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 300)               8700      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 99,301\n",
      "Trainable params: 99,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(tf.keras.Input(shape=(28,)))\n",
    "\n",
    "model.add(Dense(300,activation = 'relu',kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay),kernel_initializer = initializer_hidden_layers))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model1.add(Dense(300,activation = 'relu',kernel_initializer = initializer_hidden_layers ,kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model1.add(Dense(300,activation = 'relu', kernel_initializer = initializer_hidden_layers, kernel_regularizer=l2(weights_decay), bias_regularizer=l2(weights_decay)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model1.add(Dense(1,activation = 'sigmoid',kernel_initializer = initializer_output))\n",
    "\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "model1.compile(optimizer = optimizers.Adadelta(), loss = 'binary_crossentropy', metrics = ['accuracy',tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7699997 samples, validate on 3300000 samples\n",
      "Epoch 1/10\n",
      "7699997/7699997 [==============================] - 57s 7us/sample - loss: 0.6932 - accuracy: 0.5324 - auc_4: 0.5590 - val_loss: 0.6908 - val_accuracy: 0.5428 - val_auc_4: 0.6200\n",
      "Epoch 2/10\n",
      "7699997/7699997 [==============================] - 55s 7us/sample - loss: 0.6871 - accuracy: 0.5691 - auc_4: 0.6326 - val_loss: 0.6822 - val_accuracy: 0.5936 - val_auc_4: 0.6451\n",
      "Epoch 3/10\n",
      "7699997/7699997 [==============================] - 55s 7us/sample - loss: 0.6743 - accuracy: 0.6070 - auc_4: 0.6509 - val_loss: 0.6653 - val_accuracy: 0.6162 - val_auc_4: 0.6606\n",
      "Epoch 4/10\n",
      "7697000/7699997 [============================>.] - ETA: 0s - loss: 0.6563 - accuracy: 0.6225 - auc_4: 0.6684"
     ]
    }
   ],
   "source": [
    "# Fitting the ANN\n",
    "model1.fit(x_train_split, y_train_split, batch_size = batch_size, validation_data=(x_test_split,y_test_split), epochs = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-Capstone] *",
   "language": "python",
   "name": "conda-env-.conda-Capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
