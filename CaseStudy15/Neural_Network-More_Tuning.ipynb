{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "project_folder = \".\"\n",
    "\n",
    "with open(os.path.join(project_folder,\"data\",\"train_test.pkl\"), \"rb\") as f:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.coo.coo_matrix"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.toarray()\n",
    "X_test_np = (X_test.toarray()) \n",
    "\n",
    "\n",
    "y_train_np = np.array(y_train)\n",
    "y_test_np  = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "np_data_scaled = sc.fit_transform(X_train_np)\n",
    "np_data_scaled_test = sc.transform(X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.utils import normalize, to_categorical\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 500)               34500     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 215,001\n",
      "Trainable params: 215,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(tf.keras.Input(shape=(68,)))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(rate=0.3))\n",
    "\n",
    "model.add(Dense(300))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', mode='max', min_delta = 0.0001 , patience = 10,restore_best_weights=True)\n",
    "\n",
    "callbacks_a = [earlyStopping]\n",
    "\n",
    "adam = optimizers.Adam(lr = 0.0001)\n",
    "\n",
    "model.compile(optimizer = optimizers.Adam(), loss = 'binary_crossentropy', metrics = [tf.keras.metrics.AUC(),tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128000 samples, validate on 32000 samples\n",
      "Epoch 1/1000\n",
      "128000/128000 [==============================] - 9s 73us/sample - loss: 0.3456 - auc: 0.9239 - recall: 0.7838 - val_loss: 0.2466 - val_auc: 0.9617 - val_recall: 0.8773\n",
      "Epoch 2/1000\n",
      "128000/128000 [==============================] - 9s 70us/sample - loss: 0.2372 - auc: 0.9646 - recall: 0.8730 - val_loss: 0.1905 - val_auc: 0.9762 - val_recall: 0.9039\n",
      "Epoch 3/1000\n",
      "128000/128000 [==============================] - 9s 70us/sample - loss: 0.2030 - auc: 0.9738 - recall: 0.8967 - val_loss: 0.1906 - val_auc: 0.9764 - val_recall: 0.9145\n",
      "Epoch 4/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1863 - auc: 0.9778 - recall: 0.9091 - val_loss: 0.1679 - val_auc: 0.9812 - val_recall: 0.9230\n",
      "Epoch 5/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1733 - auc: 0.9806 - recall: 0.9149 - val_loss: 0.1507 - val_auc: 0.9845 - val_recall: 0.9305\n",
      "Epoch 6/1000\n",
      "128000/128000 [==============================] - 9s 67us/sample - loss: 0.1667 - auc: 0.9820 - recall: 0.9207 - val_loss: 0.1506 - val_auc: 0.9844 - val_recall: 0.9279\n",
      "Epoch 7/1000\n",
      "128000/128000 [==============================] - 9s 69us/sample - loss: 0.1582 - auc: 0.9834 - recall: 0.9238 - val_loss: 0.1383 - val_auc: 0.9864 - val_recall: 0.9284\n",
      "Epoch 8/1000\n",
      "128000/128000 [==============================] - 9s 69us/sample - loss: 0.1544 - auc: 0.9841 - recall: 0.9274 - val_loss: 0.1413 - val_auc: 0.9861 - val_recall: 0.9399\n",
      "Epoch 9/1000\n",
      "128000/128000 [==============================] - 9s 68us/sample - loss: 0.1503 - auc: 0.9849 - recall: 0.9296 - val_loss: 0.1335 - val_auc: 0.9876 - val_recall: 0.9272\n",
      "Epoch 10/1000\n",
      "128000/128000 [==============================] - 9s 67us/sample - loss: 0.1476 - auc: 0.9854 - recall: 0.9310 - val_loss: 0.1308 - val_auc: 0.9879 - val_recall: 0.9333\n",
      "Epoch 11/1000\n",
      "128000/128000 [==============================] - 9s 67us/sample - loss: 0.1432 - auc: 0.9861 - recall: 0.9327 - val_loss: 0.1367 - val_auc: 0.9871 - val_recall: 0.9262\n",
      "Epoch 12/1000\n",
      "128000/128000 [==============================] - 9s 68us/sample - loss: 0.1419 - auc: 0.9863 - recall: 0.9355 - val_loss: 0.1279 - val_auc: 0.9883 - val_recall: 0.9392\n",
      "Epoch 13/1000\n",
      "128000/128000 [==============================] - 9s 67us/sample - loss: 0.1365 - auc: 0.9871 - recall: 0.9373 - val_loss: 0.1265 - val_auc: 0.9880 - val_recall: 0.9492\n",
      "Epoch 14/1000\n",
      "128000/128000 [==============================] - 8s 66us/sample - loss: 0.1385 - auc: 0.9869 - recall: 0.9367 - val_loss: 0.1219 - val_auc: 0.9890 - val_recall: 0.9464\n",
      "Epoch 15/1000\n",
      "128000/128000 [==============================] - 8s 66us/sample - loss: 0.1337 - auc: 0.9876 - recall: 0.9401 - val_loss: 0.1231 - val_auc: 0.9890 - val_recall: 0.9388\n",
      "Epoch 16/1000\n",
      "128000/128000 [==============================] - 8s 66us/sample - loss: 0.1318 - auc: 0.9879 - recall: 0.9409 - val_loss: 0.1228 - val_auc: 0.9887 - val_recall: 0.9576\n",
      "Epoch 17/1000\n",
      "128000/128000 [==============================] - 8s 66us/sample - loss: 0.1309 - auc: 0.9880 - recall: 0.9402 - val_loss: 0.1238 - val_auc: 0.9887 - val_recall: 0.9496\n",
      "Epoch 18/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1285 - auc: 0.9883 - recall: 0.9430 - val_loss: 0.1217 - val_auc: 0.9892 - val_recall: 0.9420\n",
      "Epoch 19/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1274 - auc: 0.9886 - recall: 0.9419 - val_loss: 0.1227 - val_auc: 0.9888 - val_recall: 0.9467\n",
      "Epoch 20/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1253 - auc: 0.9888 - recall: 0.9444 - val_loss: 0.1240 - val_auc: 0.9887 - val_recall: 0.9395\n",
      "Epoch 21/1000\n",
      "128000/128000 [==============================] - 8s 66us/sample - loss: 0.1266 - auc: 0.9886 - recall: 0.9442 - val_loss: 0.1152 - val_auc: 0.9897 - val_recall: 0.9495\n",
      "Epoch 22/1000\n",
      "128000/128000 [==============================] - 8s 66us/sample - loss: 0.1236 - auc: 0.9890 - recall: 0.9446 - val_loss: 0.1180 - val_auc: 0.9899 - val_recall: 0.9358\n",
      "Epoch 23/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1222 - auc: 0.9893 - recall: 0.9453 - val_loss: 0.1161 - val_auc: 0.9899 - val_recall: 0.9551\n",
      "Epoch 24/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1231 - auc: 0.9891 - recall: 0.9448 - val_loss: 0.1099 - val_auc: 0.9907 - val_recall: 0.9460\n",
      "Epoch 25/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1201 - auc: 0.9896 - recall: 0.9468 - val_loss: 0.1158 - val_auc: 0.9897 - val_recall: 0.9566\n",
      "Epoch 26/1000\n",
      "128000/128000 [==============================] - 9s 67us/sample - loss: 0.1191 - auc: 0.9896 - recall: 0.9472 - val_loss: 0.1212 - val_auc: 0.9888 - val_recall: 0.9483\n",
      "Epoch 27/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1167 - auc: 0.9900 - recall: 0.9485 - val_loss: 0.1164 - val_auc: 0.9897 - val_recall: 0.9458\n",
      "Epoch 28/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1175 - auc: 0.9899 - recall: 0.9485 - val_loss: 0.1095 - val_auc: 0.9907 - val_recall: 0.9454\n",
      "Epoch 29/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1164 - auc: 0.9900 - recall: 0.9486 - val_loss: 0.1160 - val_auc: 0.9898 - val_recall: 0.9572\n",
      "Epoch 30/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1156 - auc: 0.9901 - recall: 0.9498 - val_loss: 0.1103 - val_auc: 0.9907 - val_recall: 0.9468\n",
      "Epoch 31/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1133 - auc: 0.9905 - recall: 0.9499 - val_loss: 0.1157 - val_auc: 0.9899 - val_recall: 0.9498\n",
      "Epoch 32/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1150 - auc: 0.9903 - recall: 0.9503 - val_loss: 0.1117 - val_auc: 0.9903 - val_recall: 0.9449\n",
      "Epoch 33/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1124 - auc: 0.9907 - recall: 0.9507 - val_loss: 0.1109 - val_auc: 0.9900 - val_recall: 0.9478\n",
      "Epoch 34/1000\n",
      "128000/128000 [==============================] - 8s 65us/sample - loss: 0.1124 - auc: 0.9906 - recall: 0.9500 - val_loss: 0.1180 - val_auc: 0.9896 - val_recall: 0.9387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a5fd63d50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN\n",
    "model.fit(np_data_scaled, y_train_np, batch_size = 256, validation_data=(np_data_scaled_test,y_test_np), epochs = 1000 , callbacks = callbacks_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:35<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "probas = list(np.arange(0.01, 0.25, 0.01))\n",
    "\n",
    "def calcCost(model, X, y, proba, fp = 10, fn = 500):\n",
    "    y_hat_proba = model.predict(X,use_multiprocessing=True)\n",
    "    y_hat = (y_hat_proba > proba).astype(int)\n",
    "    mt = metrics.confusion_matrix(y, y_hat)\n",
    "    acc = metrics.accuracy_score(y, y_hat)\n",
    "    precision = metrics.precision_score(y, y_hat)\n",
    "    return {\n",
    "        \"probability\":proba,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"fn\":mt[1,0],\n",
    "        \"fn_cost\":mt[1,0]*fn,\n",
    "        \"fp\":mt[0,1],\n",
    "        \"fp_cost\":mt[0,1]*fp\n",
    "    }\n",
    "\n",
    "cost = [ calcCost(model, np_data_scaled_test, y_test, proba) for proba in tqdm(probas) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>fn</th>\n",
       "      <th>fn_cost</th>\n",
       "      <th>fp</th>\n",
       "      <th>fp_cost</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.860094</td>\n",
       "      <td>0.742894</td>\n",
       "      <td>90</td>\n",
       "      <td>45000</td>\n",
       "      <td>4387</td>\n",
       "      <td>43870</td>\n",
       "      <td>88870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.883906</td>\n",
       "      <td>0.778235</td>\n",
       "      <td>108</td>\n",
       "      <td>54000</td>\n",
       "      <td>3607</td>\n",
       "      <td>36070</td>\n",
       "      <td>90070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.898062</td>\n",
       "      <td>0.800950</td>\n",
       "      <td>119</td>\n",
       "      <td>59500</td>\n",
       "      <td>3143</td>\n",
       "      <td>31430</td>\n",
       "      <td>90930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.817905</td>\n",
       "      <td>131</td>\n",
       "      <td>65500</td>\n",
       "      <td>2813</td>\n",
       "      <td>28130</td>\n",
       "      <td>93630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.815250</td>\n",
       "      <td>0.684903</td>\n",
       "      <td>72</td>\n",
       "      <td>36000</td>\n",
       "      <td>5840</td>\n",
       "      <td>58400</td>\n",
       "      <td>94400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   probability  accuracy  precision   fn  fn_cost    fp  fp_cost  total_cost\n",
       "1         0.02  0.860094   0.742894   90    45000  4387    43870       88870\n",
       "2         0.03  0.883906   0.778235  108    54000  3607    36070       90070\n",
       "3         0.04  0.898062   0.800950  119    59500  3143    31430       90930\n",
       "4         0.05  0.908000   0.817905  131    65500  2813    28130       93630\n",
       "0         0.01  0.815250   0.684903   72    36000  5840    58400       94400"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cost_df = pd.DataFrame(cost)\n",
    "\n",
    "cost_df[\"total_cost\"] = cost_df.fn_cost + cost_df.fp_cost\n",
    "\n",
    "cost_df.sort_values(by=[\"total_cost\"], ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/model_85.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:QTW] *",
   "language": "python",
   "name": "conda-env-QTW-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
