{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "project_folder = \".\"\n",
    "\n",
    "with open(os.path.join(project_folder,\"data\",\"train_test.pkl\"), \"rb\") as f:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.coo.coo_matrix"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.toarray()\n",
    "X_test_np = (X_test.toarray()) \n",
    "\n",
    "\n",
    "y_train_np = np.array(y_train)\n",
    "y_test_np  = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "np_data_scaled = sc.fit_transform(X_train_np)\n",
    "np_data_scaled_test = sc.transform(X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.utils import normalize, to_categorical\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Y variables to Categorical\n",
    "\n",
    "y_train_categorical = to_categorical(y_train_np)\n",
    "y_test_categorical = to_categorical(y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 500)               34500     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 215,102\n",
      "Trainable params: 215,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(tf.keras.Input(shape=(68,)))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(rate=0.3))\n",
    "\n",
    "model.add(Dense(300))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "model.add(Dense(2,activation = 'sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', mode='max', min_delta = 0.0001 , patience = 10,restore_best_weights=True)\n",
    "\n",
    "callbacks_a = [earlyStopping]\n",
    "\n",
    "adam = optimizers.Adam(lr = 0.0001)\n",
    "\n",
    "model.compile(optimizer = optimizers.Adam(), loss = 'binary_crossentropy', metrics = [tf.keras.metrics.AUC(),tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128000 samples, validate on 32000 samples\n",
      "Epoch 1/1000\n",
      "128000/128000 [==============================] - 10s 78us/sample - loss: 0.3453 - auc: 0.9276 - recall: 0.8476 - val_loss: 0.2498 - val_auc: 0.9622 - val_recall: 0.9003\n",
      "Epoch 2/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.2343 - auc: 0.9673 - recall: 0.9074 - val_loss: 0.1959 - val_auc: 0.9764 - val_recall: 0.9231\n",
      "Epoch 3/1000\n",
      "128000/128000 [==============================] - 9s 70us/sample - loss: 0.1988 - auc: 0.9761 - recall: 0.9241 - val_loss: 0.1790 - val_auc: 0.9802 - val_recall: 0.9323\n",
      "Epoch 4/1000\n",
      "128000/128000 [==============================] - 9s 70us/sample - loss: 0.1783 - auc: 0.9804 - recall: 0.9343 - val_loss: 0.1614 - val_auc: 0.9836 - val_recall: 0.9410\n",
      "Epoch 5/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1683 - auc: 0.9825 - recall: 0.9391 - val_loss: 0.1465 - val_auc: 0.9863 - val_recall: 0.9476\n",
      "Epoch 6/1000\n",
      "128000/128000 [==============================] - 9s 70us/sample - loss: 0.1575 - auc: 0.9846 - recall: 0.9433 - val_loss: 0.1489 - val_auc: 0.9857 - val_recall: 0.9473\n",
      "Epoch 7/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1525 - auc: 0.9854 - recall: 0.9457 - val_loss: 0.1407 - val_auc: 0.9872 - val_recall: 0.9509\n",
      "Epoch 8/1000\n",
      "128000/128000 [==============================] - 9s 70us/sample - loss: 0.1460 - auc: 0.9863 - recall: 0.9488 - val_loss: 0.1385 - val_auc: 0.9874 - val_recall: 0.9515\n",
      "Epoch 9/1000\n",
      "128000/128000 [==============================] - 9s 70us/sample - loss: 0.1408 - auc: 0.9872 - recall: 0.9513 - val_loss: 0.1339 - val_auc: 0.9880 - val_recall: 0.9532\n",
      "Epoch 10/1000\n",
      "128000/128000 [==============================] - 9s 70us/sample - loss: 0.1382 - auc: 0.9875 - recall: 0.9513 - val_loss: 0.1292 - val_auc: 0.9887 - val_recall: 0.9551\n",
      "Epoch 11/1000\n",
      "128000/128000 [==============================] - 9s 70us/sample - loss: 0.1334 - auc: 0.9884 - recall: 0.9541 - val_loss: 0.1301 - val_auc: 0.9887 - val_recall: 0.9550\n",
      "Epoch 12/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1315 - auc: 0.9886 - recall: 0.9546 - val_loss: 0.1291 - val_auc: 0.9887 - val_recall: 0.9557\n",
      "Epoch 13/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1288 - auc: 0.9889 - recall: 0.9563 - val_loss: 0.1240 - val_auc: 0.9893 - val_recall: 0.9581\n",
      "Epoch 14/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1273 - auc: 0.9890 - recall: 0.9570 - val_loss: 0.1210 - val_auc: 0.9899 - val_recall: 0.9578\n",
      "Epoch 15/1000\n",
      "128000/128000 [==============================] - 9s 70us/sample - loss: 0.1240 - auc: 0.9895 - recall: 0.9582 - val_loss: 0.1161 - val_auc: 0.9905 - val_recall: 0.9617\n",
      "Epoch 16/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1234 - auc: 0.9897 - recall: 0.9580 - val_loss: 0.1191 - val_auc: 0.9900 - val_recall: 0.9597\n",
      "Epoch 17/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1211 - auc: 0.9900 - recall: 0.9587 - val_loss: 0.1181 - val_auc: 0.9903 - val_recall: 0.9611\n",
      "Epoch 18/1000\n",
      "128000/128000 [==============================] - 9s 70us/sample - loss: 0.1188 - auc: 0.9903 - recall: 0.9602 - val_loss: 0.1126 - val_auc: 0.9910 - val_recall: 0.9640\n",
      "Epoch 19/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1156 - auc: 0.9907 - recall: 0.9618 - val_loss: 0.1147 - val_auc: 0.9905 - val_recall: 0.9611\n",
      "Epoch 20/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1153 - auc: 0.9908 - recall: 0.9620 - val_loss: 0.1134 - val_auc: 0.9907 - val_recall: 0.9635\n",
      "Epoch 21/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1146 - auc: 0.9908 - recall: 0.9624 - val_loss: 0.1142 - val_auc: 0.9906 - val_recall: 0.9625\n",
      "Epoch 22/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1125 - auc: 0.9910 - recall: 0.9628 - val_loss: 0.1074 - val_auc: 0.9913 - val_recall: 0.9654\n",
      "Epoch 23/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1097 - auc: 0.9913 - recall: 0.9640 - val_loss: 0.1117 - val_auc: 0.9909 - val_recall: 0.9640\n",
      "Epoch 24/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1081 - auc: 0.9916 - recall: 0.9645 - val_loss: 0.1116 - val_auc: 0.9908 - val_recall: 0.9635\n",
      "Epoch 25/1000\n",
      "128000/128000 [==============================] - 10s 80us/sample - loss: 0.1091 - auc: 0.9915 - recall: 0.9641 - val_loss: 0.1133 - val_auc: 0.9907 - val_recall: 0.9627\n",
      "Epoch 26/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1072 - auc: 0.9916 - recall: 0.9649 - val_loss: 0.1104 - val_auc: 0.9910 - val_recall: 0.9642\n",
      "Epoch 27/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1041 - auc: 0.9919 - recall: 0.9662 - val_loss: 0.1090 - val_auc: 0.9913 - val_recall: 0.9654\n",
      "Epoch 28/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1057 - auc: 0.9918 - recall: 0.9651 - val_loss: 0.1062 - val_auc: 0.9914 - val_recall: 0.9675\n",
      "Epoch 29/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1041 - auc: 0.9920 - recall: 0.9665 - val_loss: 0.1083 - val_auc: 0.9912 - val_recall: 0.9654\n",
      "Epoch 30/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1016 - auc: 0.9923 - recall: 0.9669 - val_loss: 0.1028 - val_auc: 0.9919 - val_recall: 0.9669\n",
      "Epoch 31/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1012 - auc: 0.9923 - recall: 0.9672 - val_loss: 0.1009 - val_auc: 0.9921 - val_recall: 0.9687\n",
      "Epoch 32/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.0999 - auc: 0.9924 - recall: 0.9680 - val_loss: 0.1019 - val_auc: 0.9920 - val_recall: 0.9689\n",
      "Epoch 33/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.0998 - auc: 0.9924 - recall: 0.9679 - val_loss: 0.1056 - val_auc: 0.9917 - val_recall: 0.9658\n",
      "Epoch 34/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.1001 - auc: 0.9925 - recall: 0.9673 - val_loss: 0.1036 - val_auc: 0.9920 - val_recall: 0.9676\n",
      "Epoch 35/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.0989 - auc: 0.9927 - recall: 0.9676 - val_loss: 0.1017 - val_auc: 0.9920 - val_recall: 0.9673\n",
      "Epoch 36/1000\n",
      "128000/128000 [==============================] - 9s 72us/sample - loss: 0.0975 - auc: 0.9927 - recall: 0.9691 - val_loss: 0.0995 - val_auc: 0.9923 - val_recall: 0.9693\n",
      "Epoch 37/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.0966 - auc: 0.9928 - recall: 0.9688 - val_loss: 0.0996 - val_auc: 0.9922 - val_recall: 0.9693\n",
      "Epoch 38/1000\n",
      "128000/128000 [==============================] - 9s 72us/sample - loss: 0.0963 - auc: 0.9929 - recall: 0.9689 - val_loss: 0.1020 - val_auc: 0.9920 - val_recall: 0.9676\n",
      "Epoch 39/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.0956 - auc: 0.9929 - recall: 0.9695 - val_loss: 0.1054 - val_auc: 0.9915 - val_recall: 0.9671\n",
      "Epoch 40/1000\n",
      "128000/128000 [==============================] - 9s 70us/sample - loss: 0.0938 - auc: 0.9932 - recall: 0.9695 - val_loss: 0.1038 - val_auc: 0.9918 - val_recall: 0.9678\n",
      "Epoch 41/1000\n",
      "128000/128000 [==============================] - 9s 73us/sample - loss: 0.0948 - auc: 0.9930 - recall: 0.9697 - val_loss: 0.1010 - val_auc: 0.9921 - val_recall: 0.9694\n",
      "Epoch 42/1000\n",
      "128000/128000 [==============================] - 9s 72us/sample - loss: 0.0930 - auc: 0.9934 - recall: 0.9701 - val_loss: 0.1042 - val_auc: 0.9920 - val_recall: 0.9677\n",
      "Epoch 43/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.0923 - auc: 0.9932 - recall: 0.9704 - val_loss: 0.1001 - val_auc: 0.9922 - val_recall: 0.9696\n",
      "Epoch 44/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.0914 - auc: 0.9934 - recall: 0.9707 - val_loss: 0.1018 - val_auc: 0.9920 - val_recall: 0.9691\n",
      "Epoch 45/1000\n",
      "128000/128000 [==============================] - 9s 72us/sample - loss: 0.0907 - auc: 0.9934 - recall: 0.9708 - val_loss: 0.1010 - val_auc: 0.9919 - val_recall: 0.9682\n",
      "Epoch 46/1000\n",
      "128000/128000 [==============================] - 9s 71us/sample - loss: 0.0914 - auc: 0.9933 - recall: 0.9705 - val_loss: 0.1064 - val_auc: 0.9916 - val_recall: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a55fd4cd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN\n",
    "model.fit(np_data_scaled, y_train_categorical, batch_size = 256, validation_data=(np_data_scaled_test,y_test_categorical), epochs = 1000 , callbacks = callbacks_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np_data_scaled_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9897516e-01, 1.0997653e-03],\n",
       "       [6.1494410e-03, 9.9467844e-01],\n",
       "       [5.6259429e-01, 4.0306184e-01],\n",
       "       ...,\n",
       "       [9.9405563e-01, 5.0358474e-03],\n",
       "       [9.9765551e-01, 4.1653216e-03],\n",
       "       [9.9942595e-01, 4.7898293e-04]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np_data_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:57<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "probas = list(np.arange(0.01, 0.4, 0.01))\n",
    "\n",
    "def calcCost(model, X, y, proba, fp = 10, fn = 500):\n",
    "    y_hat_proba = model.predict(X,use_multiprocessing=True)\n",
    "    y_hat = (y_hat_proba[:,1] > proba).astype(int)\n",
    "    mt = metrics.confusion_matrix(y, y_hat)\n",
    "    acc = metrics.accuracy_score(y, y_hat)\n",
    "    precision = metrics.precision_score(y, y_hat)\n",
    "    return {\n",
    "        \"probability\":proba,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"fn\":mt[1,0],\n",
    "        \"fn_cost\":mt[1,0]*fn,\n",
    "        \"fp\":mt[0,1],\n",
    "        \"fp_cost\":mt[0,1]*fp\n",
    "    }\n",
    "\n",
    "cost = [ calcCost(model, np_data_scaled_test, y_test, proba) for proba in tqdm(probas) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>fn</th>\n",
       "      <th>fn_cost</th>\n",
       "      <th>fp</th>\n",
       "      <th>fp_cost</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.899344</td>\n",
       "      <td>0.802114</td>\n",
       "      <td>95</td>\n",
       "      <td>47500</td>\n",
       "      <td>3126</td>\n",
       "      <td>31260</td>\n",
       "      <td>78760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.908531</td>\n",
       "      <td>0.817695</td>\n",
       "      <td>104</td>\n",
       "      <td>52000</td>\n",
       "      <td>2823</td>\n",
       "      <td>28230</td>\n",
       "      <td>80230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.885813</td>\n",
       "      <td>0.780576</td>\n",
       "      <td>91</td>\n",
       "      <td>45500</td>\n",
       "      <td>3563</td>\n",
       "      <td>35630</td>\n",
       "      <td>81130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.915531</td>\n",
       "      <td>0.829999</td>\n",
       "      <td>111</td>\n",
       "      <td>55500</td>\n",
       "      <td>2592</td>\n",
       "      <td>25920</td>\n",
       "      <td>81420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.863094</td>\n",
       "      <td>0.746923</td>\n",
       "      <td>84</td>\n",
       "      <td>42000</td>\n",
       "      <td>4297</td>\n",
       "      <td>42970</td>\n",
       "      <td>84970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   probability  accuracy  precision   fn  fn_cost    fp  fp_cost  total_cost\n",
       "3         0.04  0.899344   0.802114   95    47500  3126    31260       78760\n",
       "4         0.05  0.908531   0.817695  104    52000  2823    28230       80230\n",
       "2         0.03  0.885813   0.780576   91    45500  3563    35630       81130\n",
       "5         0.06  0.915531   0.829999  111    55500  2592    25920       81420\n",
       "1         0.02  0.863094   0.746923   84    42000  4297    42970       84970"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cost_df = pd.DataFrame(cost)\n",
    "\n",
    "cost_df[\"total_cost\"] = cost_df.fn_cost + cost_df.fp_cost\n",
    "\n",
    "cost_df.sort_values(by=[\"total_cost\"], ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/model_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:QTW] *",
   "language": "python",
   "name": "conda-env-QTW-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
